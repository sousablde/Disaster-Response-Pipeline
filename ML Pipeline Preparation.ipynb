{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sousa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sousa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.base import BaseEstimator\n",
    "import pickle\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "nltk.download(['punkt', 'wordnet'])\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "#imblearn\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///messages.db')\n",
    "df = pd.read_sql_table(\"messages\", con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>message</th>\n",
       "      <th>original</th>\n",
       "      <th>genre</th>\n",
       "      <th>related</th>\n",
       "      <th>request</th>\n",
       "      <th>offer</th>\n",
       "      <th>aid_related</th>\n",
       "      <th>medical_help</th>\n",
       "      <th>medical_products</th>\n",
       "      <th>...</th>\n",
       "      <th>aid_centers</th>\n",
       "      <th>other_infrastructure</th>\n",
       "      <th>weather_related</th>\n",
       "      <th>floods</th>\n",
       "      <th>storm</th>\n",
       "      <th>fire</th>\n",
       "      <th>earthquake</th>\n",
       "      <th>cold</th>\n",
       "      <th>other_weather</th>\n",
       "      <th>direct_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Weather update - a cold front from Cuba that c...</td>\n",
       "      <td>Un front froid se retrouve sur Cuba ce matin. ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>Is the Hurricane over or is it not over</td>\n",
       "      <td>Cyclone nan fini osinon li pa fini</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>Looking for someone but no name</td>\n",
       "      <td>Patnm, di Maryani relem pou li banm nouvel li ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>says: west side of Haiti, rest of the country ...</td>\n",
       "      <td>facade ouest d Haiti et le reste du pays aujou...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            message  \\\n",
       "0   2  Weather update - a cold front from Cuba that c...   \n",
       "1   7            Is the Hurricane over or is it not over   \n",
       "2   8                    Looking for someone but no name   \n",
       "3   9  UN reports Leogane 80-90 destroyed. Only Hospi...   \n",
       "4  12  says: west side of Haiti, rest of the country ...   \n",
       "\n",
       "                                            original   genre  related  \\\n",
       "0  Un front froid se retrouve sur Cuba ce matin. ...  direct        1   \n",
       "1                 Cyclone nan fini osinon li pa fini  direct        1   \n",
       "2  Patnm, di Maryani relem pou li banm nouvel li ...  direct        1   \n",
       "3  UN reports Leogane 80-90 destroyed. Only Hospi...  direct        1   \n",
       "4  facade ouest d Haiti et le reste du pays aujou...  direct        1   \n",
       "\n",
       "   request  offer  aid_related  medical_help  medical_products  ...  \\\n",
       "0        0      0            0             0                 0  ...   \n",
       "1        0      0            1             0                 0  ...   \n",
       "2        0      0            0             0                 0  ...   \n",
       "3        1      0            1             0                 1  ...   \n",
       "4        0      0            0             0                 0  ...   \n",
       "\n",
       "   aid_centers  other_infrastructure  weather_related  floods  storm  fire  \\\n",
       "0            0                     0                0       0      0     0   \n",
       "1            0                     0                1       0      1     0   \n",
       "2            0                     0                0       0      0     0   \n",
       "3            0                     0                0       0      0     0   \n",
       "4            0                     0                0       0      0     0   \n",
       "\n",
       "   earthquake  cold  other_weather  direct_report  \n",
       "0           0     0              0              0  \n",
       "1           0     0              0              0  \n",
       "2           0     0              0              0  \n",
       "3           0     0              0              0  \n",
       "4           0     0              0              0  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on this quick check most of the data is very imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"message\"]\n",
    "y = df.drop(['message', 'genre', 'id', 'original'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    '''\n",
    "    Receives text related data and processes it\n",
    "    Args: text related data (columns)\n",
    "    Returns: tokenized text\n",
    "    '''\n",
    "    # get list of all urls using regex\n",
    "    detected_urls = re.findall(url_regex, text) \n",
    "    \n",
    "    # replace each url in text string with placeholder\n",
    "    for url in detected_urls:\n",
    "        text = text.replace(url, \"urlplaceholder\")\n",
    "\n",
    "    # tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # initiate lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # iterate through each token\n",
    "    clean_tokens = []\n",
    "    for tok in tokens:\n",
    "        \n",
    "        # lemmatize, normalize case, and remove leading/trailing white space\n",
    "        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n",
    "        clean_tokens.append(clean_tok)\n",
    "\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_tester(X, y):\n",
    "    '''\n",
    "    Function to create list of fitted models\n",
    "    Args: training data X and y\n",
    "    returns: list of the selected fitted models\n",
    "    '''\n",
    "    pipe_1 = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "    ])\n",
    "    \n",
    "    pipe_2 = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', MultiOutputClassifier(ExtraTreesClassifier()))\n",
    "    ])\n",
    "    \n",
    "    pipe_3 = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', MultiOutputClassifier(GradientBoostingClassifier()))\n",
    "    ])\n",
    "    \n",
    "    pipe_4 = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', MultiOutputClassifier(AdaBoostClassifier()))\n",
    "    ])\n",
    "    \n",
    "    pipe_5 = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', MultiOutputClassifier(SVC()))\n",
    "    ])\n",
    "    \n",
    "    pips = [pipe_1, pipe_2, pipe_3, pipe_4, pipe_5]\n",
    "    pip_names = ['RandomForestClassifier', 'ExtraTreesClassifier', 'GradientBoostingClassifier', \n",
    "                 'AdaBoostClassifier', 'SVC']\n",
    "    \n",
    "    model_fits = []\n",
    "    for i in range(len(pips)):\n",
    "        print('Model: ', pip_names[i])\n",
    "        print(pips[i].get_params())\n",
    "        mdl = pips[i].fit(X, y)\n",
    "        model_fits.append(mdl)\n",
    "        \n",
    "    return model_fits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size = 0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  RandomForestClassifier\n",
      "{'memory': None, 'steps': [('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=<function tokenize at 0x0000021C73D19F78>,\n",
      "                vocabulary=None)), ('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)), ('clf', MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True,\n",
      "                                                       class_weight=None,\n",
      "                                                       criterion='gini',\n",
      "                                                       max_depth=None,\n",
      "                                                       max_features='auto',\n",
      "                                                       max_leaf_nodes=None,\n",
      "                                                       min_impurity_decrease=0.0,\n",
      "                                                       min_impurity_split=None,\n",
      "                                                       min_samples_leaf=1,\n",
      "                                                       min_samples_split=2,\n",
      "                                                       min_weight_fraction_leaf=0.0,\n",
      "                                                       n_estimators='warn',\n",
      "                                                       n_jobs=None,\n",
      "                                                       oob_score=False,\n",
      "                                                       random_state=None,\n",
      "                                                       verbose=0,\n",
      "                                                       warm_start=False),\n",
      "                      n_jobs=None))], 'verbose': False, 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=<function tokenize at 0x0000021C73D19F78>,\n",
      "                vocabulary=None), 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True), 'clf': MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True,\n",
      "                                                       class_weight=None,\n",
      "                                                       criterion='gini',\n",
      "                                                       max_depth=None,\n",
      "                                                       max_features='auto',\n",
      "                                                       max_leaf_nodes=None,\n",
      "                                                       min_impurity_decrease=0.0,\n",
      "                                                       min_impurity_split=None,\n",
      "                                                       min_samples_leaf=1,\n",
      "                                                       min_samples_split=2,\n",
      "                                                       min_weight_fraction_leaf=0.0,\n",
      "                                                       n_estimators='warn',\n",
      "                                                       n_jobs=None,\n",
      "                                                       oob_score=False,\n",
      "                                                       random_state=None,\n",
      "                                                       verbose=0,\n",
      "                                                       warm_start=False),\n",
      "                      n_jobs=None), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': <function tokenize at 0x0000021C73D19F78>, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'clf__estimator__bootstrap': True, 'clf__estimator__class_weight': None, 'clf__estimator__criterion': 'gini', 'clf__estimator__max_depth': None, 'clf__estimator__max_features': 'auto', 'clf__estimator__max_leaf_nodes': None, 'clf__estimator__min_impurity_decrease': 0.0, 'clf__estimator__min_impurity_split': None, 'clf__estimator__min_samples_leaf': 1, 'clf__estimator__min_samples_split': 2, 'clf__estimator__min_weight_fraction_leaf': 0.0, 'clf__estimator__n_estimators': 'warn', 'clf__estimator__n_jobs': None, 'clf__estimator__oob_score': False, 'clf__estimator__random_state': None, 'clf__estimator__verbose': 0, 'clf__estimator__warm_start': False, 'clf__estimator': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators='warn',\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False), 'clf__n_jobs': None}\n",
      "Model:  ExtraTreesClassifier\n",
      "{'memory': None, 'steps': [('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=<function tokenize at 0x0000021C73D19F78>,\n",
      "                vocabulary=None)), ('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)), ('clf', MultiOutputClassifier(estimator=ExtraTreesClassifier(bootstrap=False,\n",
      "                                                     class_weight=None,\n",
      "                                                     criterion='gini',\n",
      "                                                     max_depth=None,\n",
      "                                                     max_features='auto',\n",
      "                                                     max_leaf_nodes=None,\n",
      "                                                     min_impurity_decrease=0.0,\n",
      "                                                     min_impurity_split=None,\n",
      "                                                     min_samples_leaf=1,\n",
      "                                                     min_samples_split=2,\n",
      "                                                     min_weight_fraction_leaf=0.0,\n",
      "                                                     n_estimators='warn',\n",
      "                                                     n_jobs=None,\n",
      "                                                     oob_score=False,\n",
      "                                                     random_state=None,\n",
      "                                                     verbose=0,\n",
      "                                                     warm_start=False),\n",
      "                      n_jobs=None))], 'verbose': False, 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=<function tokenize at 0x0000021C73D19F78>,\n",
      "                vocabulary=None), 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True), 'clf': MultiOutputClassifier(estimator=ExtraTreesClassifier(bootstrap=False,\n",
      "                                                     class_weight=None,\n",
      "                                                     criterion='gini',\n",
      "                                                     max_depth=None,\n",
      "                                                     max_features='auto',\n",
      "                                                     max_leaf_nodes=None,\n",
      "                                                     min_impurity_decrease=0.0,\n",
      "                                                     min_impurity_split=None,\n",
      "                                                     min_samples_leaf=1,\n",
      "                                                     min_samples_split=2,\n",
      "                                                     min_weight_fraction_leaf=0.0,\n",
      "                                                     n_estimators='warn',\n",
      "                                                     n_jobs=None,\n",
      "                                                     oob_score=False,\n",
      "                                                     random_state=None,\n",
      "                                                     verbose=0,\n",
      "                                                     warm_start=False),\n",
      "                      n_jobs=None), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': <function tokenize at 0x0000021C73D19F78>, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'clf__estimator__bootstrap': False, 'clf__estimator__class_weight': None, 'clf__estimator__criterion': 'gini', 'clf__estimator__max_depth': None, 'clf__estimator__max_features': 'auto', 'clf__estimator__max_leaf_nodes': None, 'clf__estimator__min_impurity_decrease': 0.0, 'clf__estimator__min_impurity_split': None, 'clf__estimator__min_samples_leaf': 1, 'clf__estimator__min_samples_split': 2, 'clf__estimator__min_weight_fraction_leaf': 0.0, 'clf__estimator__n_estimators': 'warn', 'clf__estimator__n_jobs': None, 'clf__estimator__oob_score': False, 'clf__estimator__random_state': None, 'clf__estimator__verbose': 0, 'clf__estimator__warm_start': False, 'clf__estimator': ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "                     max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                     min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                     min_samples_leaf=1, min_samples_split=2,\n",
      "                     min_weight_fraction_leaf=0.0, n_estimators='warn',\n",
      "                     n_jobs=None, oob_score=False, random_state=None, verbose=0,\n",
      "                     warm_start=False), 'clf__n_jobs': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  GradientBoostingClassifier\n",
      "{'memory': None, 'steps': [('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=<function tokenize at 0x0000021C73D19F78>,\n",
      "                vocabulary=None)), ('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)), ('clf', MultiOutputClassifier(estimator=GradientBoostingClassifier(criterion='friedman_mse',\n",
      "                                                           init=None,\n",
      "                                                           learning_rate=0.1,\n",
      "                                                           loss='deviance',\n",
      "                                                           max_depth=3,\n",
      "                                                           max_features=None,\n",
      "                                                           max_leaf_nodes=None,\n",
      "                                                           min_impurity_decrease=0.0,\n",
      "                                                           min_impurity_split=None,\n",
      "                                                           min_samples_leaf=1,\n",
      "                                                           min_samples_split=2,\n",
      "                                                           min_weight_fraction_leaf=0.0,\n",
      "                                                           n_estimators=100,\n",
      "                                                           n_iter_no_change=None,\n",
      "                                                           presort='auto',\n",
      "                                                           random_state=None,\n",
      "                                                           subsample=1.0,\n",
      "                                                           tol=0.0001,\n",
      "                                                           validation_fraction=0.1,\n",
      "                                                           verbose=0,\n",
      "                                                           warm_start=False),\n",
      "                      n_jobs=None))], 'verbose': False, 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=<function tokenize at 0x0000021C73D19F78>,\n",
      "                vocabulary=None), 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True), 'clf': MultiOutputClassifier(estimator=GradientBoostingClassifier(criterion='friedman_mse',\n",
      "                                                           init=None,\n",
      "                                                           learning_rate=0.1,\n",
      "                                                           loss='deviance',\n",
      "                                                           max_depth=3,\n",
      "                                                           max_features=None,\n",
      "                                                           max_leaf_nodes=None,\n",
      "                                                           min_impurity_decrease=0.0,\n",
      "                                                           min_impurity_split=None,\n",
      "                                                           min_samples_leaf=1,\n",
      "                                                           min_samples_split=2,\n",
      "                                                           min_weight_fraction_leaf=0.0,\n",
      "                                                           n_estimators=100,\n",
      "                                                           n_iter_no_change=None,\n",
      "                                                           presort='auto',\n",
      "                                                           random_state=None,\n",
      "                                                           subsample=1.0,\n",
      "                                                           tol=0.0001,\n",
      "                                                           validation_fraction=0.1,\n",
      "                                                           verbose=0,\n",
      "                                                           warm_start=False),\n",
      "                      n_jobs=None), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': <function tokenize at 0x0000021C73D19F78>, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'clf__estimator__criterion': 'friedman_mse', 'clf__estimator__init': None, 'clf__estimator__learning_rate': 0.1, 'clf__estimator__loss': 'deviance', 'clf__estimator__max_depth': 3, 'clf__estimator__max_features': None, 'clf__estimator__max_leaf_nodes': None, 'clf__estimator__min_impurity_decrease': 0.0, 'clf__estimator__min_impurity_split': None, 'clf__estimator__min_samples_leaf': 1, 'clf__estimator__min_samples_split': 2, 'clf__estimator__min_weight_fraction_leaf': 0.0, 'clf__estimator__n_estimators': 100, 'clf__estimator__n_iter_no_change': None, 'clf__estimator__presort': 'auto', 'clf__estimator__random_state': None, 'clf__estimator__subsample': 1.0, 'clf__estimator__tol': 0.0001, 'clf__estimator__validation_fraction': 0.1, 'clf__estimator__verbose': 0, 'clf__estimator__warm_start': False, 'clf__estimator': GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "                           max_features=None, max_leaf_nodes=None,\n",
      "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                           min_samples_leaf=1, min_samples_split=2,\n",
      "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                           n_iter_no_change=None, presort='auto',\n",
      "                           random_state=None, subsample=1.0, tol=0.0001,\n",
      "                           validation_fraction=0.1, verbose=0,\n",
      "                           warm_start=False), 'clf__n_jobs': None}\n",
      "Model:  AdaBoostClassifier\n",
      "{'memory': None, 'steps': [('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=<function tokenize at 0x0000021C73D19F78>,\n",
      "                vocabulary=None)), ('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)), ('clf', MultiOutputClassifier(estimator=AdaBoostClassifier(algorithm='SAMME.R',\n",
      "                                                   base_estimator=None,\n",
      "                                                   learning_rate=1.0,\n",
      "                                                   n_estimators=50,\n",
      "                                                   random_state=None),\n",
      "                      n_jobs=None))], 'verbose': False, 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=<function tokenize at 0x0000021C73D19F78>,\n",
      "                vocabulary=None), 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True), 'clf': MultiOutputClassifier(estimator=AdaBoostClassifier(algorithm='SAMME.R',\n",
      "                                                   base_estimator=None,\n",
      "                                                   learning_rate=1.0,\n",
      "                                                   n_estimators=50,\n",
      "                                                   random_state=None),\n",
      "                      n_jobs=None), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': <function tokenize at 0x0000021C73D19F78>, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'clf__estimator__algorithm': 'SAMME.R', 'clf__estimator__base_estimator': None, 'clf__estimator__learning_rate': 1.0, 'clf__estimator__n_estimators': 50, 'clf__estimator__random_state': None, 'clf__estimator': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
      "                   n_estimators=50, random_state=None), 'clf__n_jobs': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  SVC\n",
      "{'memory': None, 'steps': [('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=<function tokenize at 0x0000021C73D19F78>,\n",
      "                vocabulary=None)), ('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)), ('clf', MultiOutputClassifier(estimator=SVC(C=1.0, cache_size=200, class_weight=None,\n",
      "                                    coef0=0.0, decision_function_shape='ovr',\n",
      "                                    degree=3, gamma='auto_deprecated',\n",
      "                                    kernel='rbf', max_iter=-1,\n",
      "                                    probability=False, random_state=None,\n",
      "                                    shrinking=True, tol=0.001, verbose=False),\n",
      "                      n_jobs=None))], 'verbose': False, 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=<function tokenize at 0x0000021C73D19F78>,\n",
      "                vocabulary=None), 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True), 'clf': MultiOutputClassifier(estimator=SVC(C=1.0, cache_size=200, class_weight=None,\n",
      "                                    coef0=0.0, decision_function_shape='ovr',\n",
      "                                    degree=3, gamma='auto_deprecated',\n",
      "                                    kernel='rbf', max_iter=-1,\n",
      "                                    probability=False, random_state=None,\n",
      "                                    shrinking=True, tol=0.001, verbose=False),\n",
      "                      n_jobs=None), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': <function tokenize at 0x0000021C73D19F78>, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'clf__estimator__C': 1.0, 'clf__estimator__cache_size': 200, 'clf__estimator__class_weight': None, 'clf__estimator__coef0': 0.0, 'clf__estimator__decision_function_shape': 'ovr', 'clf__estimator__degree': 3, 'clf__estimator__gamma': 'auto_deprecated', 'clf__estimator__kernel': 'rbf', 'clf__estimator__max_iter': -1, 'clf__estimator__probability': False, 'clf__estimator__random_state': None, 'clf__estimator__shrinking': True, 'clf__estimator__tol': 0.001, 'clf__estimator__verbose': False, 'clf__estimator': SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "    kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
      "    shrinking=True, tol=0.001, verbose=False), 'clf__n_jobs': None}\n"
     ]
    }
   ],
   "source": [
    "fitted_mdls = multi_tester(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your models\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = y_train.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perf_report(model, X_test, y_test):\n",
    "    '''\n",
    "    Function to return model classification reports\n",
    "    Input: Model list, and test data \n",
    "    Output: Prints the Classification report\n",
    "    '''\n",
    "    pip_names = ['RandomForestClassifier', 'ExtraTreesClassifier', 'GradientBoostingClassifier', \n",
    "             'AdaBoostClassifier', 'SVC']\n",
    "    \n",
    "    for i in range(len(model)):\n",
    "        print('______________________________Model______________________________')\n",
    "        print('______________________________', pip_names[i], '______________________________')\n",
    "        y_pred = model[i].predict(X_test)\n",
    "        print(classification_report(y_test, y_pred, target_names = target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________Model______________________________\n",
      "______________________________ RandomForestClassifier ______________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related       0.81      0.93      0.87      6534\n",
      "               request       0.83      0.37      0.51      1472\n",
      "                 offer       0.00      0.00      0.00        38\n",
      "           aid_related       0.74      0.53      0.62      3545\n",
      "          medical_help       0.65      0.10      0.17       701\n",
      "      medical_products       0.60      0.07      0.12       446\n",
      "     search_and_rescue       0.81      0.06      0.11       226\n",
      "              security       0.00      0.00      0.00       160\n",
      "              military       0.62      0.09      0.16       267\n",
      "                 water       0.80      0.18      0.29       543\n",
      "                  food       0.89      0.31      0.46       965\n",
      "               shelter       0.78      0.19      0.31       775\n",
      "              clothing       0.86      0.14      0.24       127\n",
      "                 money       0.62      0.03      0.05       191\n",
      "        missing_people       0.60      0.03      0.06       104\n",
      "              refugees       0.62      0.07      0.13       293\n",
      "                 death       0.82      0.11      0.20       406\n",
      "             other_aid       0.52      0.03      0.06      1139\n",
      "infrastructure_related       0.67      0.00      0.01       568\n",
      "             transport       0.72      0.07      0.13       407\n",
      "             buildings       0.66      0.05      0.10       441\n",
      "           electricity       0.78      0.04      0.07       185\n",
      "                 tools       0.00      0.00      0.00        53\n",
      "             hospitals       0.00      0.00      0.00        85\n",
      "                 shops       0.00      0.00      0.00        34\n",
      "           aid_centers       0.00      0.00      0.00       115\n",
      "  other_infrastructure       0.50      0.00      0.01       383\n",
      "       weather_related       0.84      0.51      0.63      2390\n",
      "                floods       0.86      0.28      0.42       693\n",
      "                 storm       0.74      0.34      0.46       812\n",
      "                  fire       0.00      0.00      0.00        90\n",
      "            earthquake       0.88      0.58      0.70       787\n",
      "                  cold       0.64      0.05      0.09       187\n",
      "         other_weather       0.53      0.02      0.03       452\n",
      "         direct_report       0.77      0.29      0.42      1694\n",
      "\n",
      "             micro avg       0.80      0.44      0.57     27308\n",
      "             macro avg       0.58      0.16      0.21     27308\n",
      "          weighted avg       0.75      0.44      0.49     27308\n",
      "           samples avg       0.65      0.41      0.46     27308\n",
      "\n",
      "______________________________Model______________________________\n",
      "______________________________ ExtraTreesClassifier ______________________________\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related       0.81      0.94      0.87      6534\n",
      "               request       0.83      0.37      0.51      1472\n",
      "                 offer       0.00      0.00      0.00        38\n",
      "           aid_related       0.75      0.52      0.61      3545\n",
      "          medical_help       0.53      0.11      0.18       701\n",
      "      medical_products       0.73      0.10      0.17       446\n",
      "     search_and_rescue       0.26      0.03      0.06       226\n",
      "              security       0.00      0.00      0.00       160\n",
      "              military       0.48      0.09      0.15       267\n",
      "                 water       0.83      0.19      0.31       543\n",
      "                  food       0.84      0.28      0.42       965\n",
      "               shelter       0.79      0.21      0.33       775\n",
      "              clothing       0.50      0.02      0.05       127\n",
      "                 money       0.53      0.05      0.09       191\n",
      "        missing_people       1.00      0.01      0.02       104\n",
      "              refugees       0.52      0.08      0.14       293\n",
      "                 death       0.80      0.09      0.16       406\n",
      "             other_aid       0.48      0.03      0.06      1139\n",
      "infrastructure_related       0.29      0.01      0.01       568\n",
      "             transport       0.56      0.06      0.10       407\n",
      "             buildings       0.66      0.10      0.18       441\n",
      "           electricity       0.56      0.08      0.13       185\n",
      "                 tools       0.00      0.00      0.00        53\n",
      "             hospitals       0.25      0.01      0.02        85\n",
      "                 shops       0.00      0.00      0.00        34\n",
      "           aid_centers       0.00      0.00      0.00       115\n",
      "  other_infrastructure       0.12      0.00      0.01       383\n",
      "       weather_related       0.83      0.53      0.65      2390\n",
      "                floods       0.86      0.28      0.42       693\n",
      "                 storm       0.74      0.35      0.48       812\n",
      "                  fire       0.25      0.01      0.02        90\n",
      "            earthquake       0.87      0.53      0.66       787\n",
      "                  cold       0.62      0.05      0.10       187\n",
      "         other_weather       0.38      0.02      0.04       452\n",
      "         direct_report       0.79      0.29      0.43      1694\n",
      "\n",
      "             micro avg       0.79      0.44      0.57     27308\n",
      "             macro avg       0.53      0.16      0.21     27308\n",
      "          weighted avg       0.72      0.44      0.50     27308\n",
      "           samples avg       0.66      0.42      0.47     27308\n",
      "\n",
      "______________________________Model______________________________\n",
      "______________________________ GradientBoostingClassifier ______________________________\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related       0.80      0.97      0.88      6534\n",
      "               request       0.84      0.52      0.64      1472\n",
      "                 offer       0.00      0.00      0.00        38\n",
      "           aid_related       0.77      0.56      0.65      3545\n",
      "          medical_help       0.64      0.22      0.33       701\n",
      "      medical_products       0.62      0.28      0.38       446\n",
      "     search_and_rescue       0.28      0.15      0.20       226\n",
      "              security       0.07      0.04      0.05       160\n",
      "              military       0.48      0.21      0.29       267\n",
      "                 water       0.73      0.64      0.68       543\n",
      "                  food       0.80      0.78      0.79       965\n",
      "               shelter       0.79      0.53      0.64       775\n",
      "              clothing       0.55      0.50      0.52       127\n",
      "                 money       0.44      0.24      0.31       191\n",
      "        missing_people       0.37      0.27      0.31       104\n",
      "              refugees       0.44      0.23      0.30       293\n",
      "                 death       0.69      0.49      0.57       406\n",
      "             other_aid       0.60      0.09      0.15      1139\n",
      "infrastructure_related       0.44      0.03      0.05       568\n",
      "             transport       0.52      0.24      0.33       407\n",
      "             buildings       0.70      0.30      0.42       441\n",
      "           electricity       0.37      0.21      0.27       185\n",
      "                 tools       0.07      0.04      0.05        53\n",
      "             hospitals       0.08      0.06      0.07        85\n",
      "                 shops       0.03      0.03      0.03        34\n",
      "           aid_centers       0.16      0.10      0.12       115\n",
      "  other_infrastructure       0.20      0.03      0.06       383\n",
      "       weather_related       0.87      0.59      0.71      2390\n",
      "                floods       0.87      0.53      0.66       693\n",
      "                 storm       0.76      0.60      0.67       812\n",
      "                  fire       0.39      0.28      0.32        90\n",
      "            earthquake       0.84      0.79      0.81       787\n",
      "                  cold       0.53      0.37      0.43       187\n",
      "         other_weather       0.52      0.11      0.18       452\n",
      "         direct_report       0.79      0.43      0.56      1694\n",
      "\n",
      "             micro avg       0.76      0.57      0.65     27308\n",
      "             macro avg       0.52      0.33      0.38     27308\n",
      "          weighted avg       0.73      0.57      0.61     27308\n",
      "           samples avg       0.65      0.50      0.52     27308\n",
      "\n",
      "______________________________Model______________________________\n",
      "______________________________ AdaBoostClassifier ______________________________\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related       0.81      0.94      0.87      6534\n",
      "               request       0.78      0.54      0.64      1472\n",
      "                 offer       0.07      0.03      0.04        38\n",
      "           aid_related       0.75      0.60      0.66      3545\n",
      "          medical_help       0.57      0.26      0.36       701\n",
      "      medical_products       0.63      0.30      0.41       446\n",
      "     search_and_rescue       0.55      0.17      0.26       226\n",
      "              security       0.20      0.03      0.04       160\n",
      "              military       0.53      0.28      0.37       267\n",
      "                 water       0.72      0.61      0.66       543\n",
      "                  food       0.80      0.68      0.74       965\n",
      "               shelter       0.76      0.57      0.65       775\n",
      "              clothing       0.65      0.40      0.50       127\n",
      "                 money       0.49      0.28      0.35       191\n",
      "        missing_people       0.65      0.11      0.18       104\n",
      "              refugees       0.60      0.29      0.39       293\n",
      "                 death       0.78      0.42      0.54       406\n",
      "             other_aid       0.55      0.16      0.25      1139\n",
      "infrastructure_related       0.46      0.09      0.15       568\n",
      "             transport       0.64      0.16      0.26       407\n",
      "             buildings       0.69      0.37      0.48       441\n",
      "           electricity       0.51      0.19      0.28       185\n",
      "                 tools       0.15      0.04      0.06        53\n",
      "             hospitals       0.27      0.08      0.13        85\n",
      "                 shops       0.08      0.03      0.04        34\n",
      "           aid_centers       0.25      0.06      0.10       115\n",
      "  other_infrastructure       0.42      0.09      0.15       383\n",
      "       weather_related       0.85      0.65      0.74      2390\n",
      "                floods       0.86      0.56      0.68       693\n",
      "                 storm       0.73      0.50      0.60       812\n",
      "                  fire       0.53      0.20      0.29        90\n",
      "            earthquake       0.87      0.75      0.80       787\n",
      "                  cold       0.74      0.34      0.47       187\n",
      "         other_weather       0.41      0.12      0.19       452\n",
      "         direct_report       0.71      0.47      0.57      1694\n",
      "\n",
      "             micro avg       0.76      0.58      0.66     27308\n",
      "             macro avg       0.57      0.33      0.40     27308\n",
      "          weighted avg       0.73      0.58      0.62     27308\n",
      "           samples avg       0.63      0.50      0.51     27308\n",
      "\n",
      "______________________________Model______________________________\n",
      "______________________________ SVC ______________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related       0.76      1.00      0.86      6534\n",
      "               request       0.00      0.00      0.00      1472\n",
      "                 offer       0.00      0.00      0.00        38\n",
      "           aid_related       0.00      0.00      0.00      3545\n",
      "          medical_help       0.00      0.00      0.00       701\n",
      "      medical_products       0.00      0.00      0.00       446\n",
      "     search_and_rescue       0.00      0.00      0.00       226\n",
      "              security       0.00      0.00      0.00       160\n",
      "              military       0.00      0.00      0.00       267\n",
      "                 water       0.00      0.00      0.00       543\n",
      "                  food       0.00      0.00      0.00       965\n",
      "               shelter       0.00      0.00      0.00       775\n",
      "              clothing       0.00      0.00      0.00       127\n",
      "                 money       0.00      0.00      0.00       191\n",
      "        missing_people       0.00      0.00      0.00       104\n",
      "              refugees       0.00      0.00      0.00       293\n",
      "                 death       0.00      0.00      0.00       406\n",
      "             other_aid       0.00      0.00      0.00      1139\n",
      "infrastructure_related       0.00      0.00      0.00       568\n",
      "             transport       0.00      0.00      0.00       407\n",
      "             buildings       0.00      0.00      0.00       441\n",
      "           electricity       0.00      0.00      0.00       185\n",
      "                 tools       0.00      0.00      0.00        53\n",
      "             hospitals       0.00      0.00      0.00        85\n",
      "                 shops       0.00      0.00      0.00        34\n",
      "           aid_centers       0.00      0.00      0.00       115\n",
      "  other_infrastructure       0.00      0.00      0.00       383\n",
      "       weather_related       0.00      0.00      0.00      2390\n",
      "                floods       0.00      0.00      0.00       693\n",
      "                 storm       0.00      0.00      0.00       812\n",
      "                  fire       0.00      0.00      0.00        90\n",
      "            earthquake       0.00      0.00      0.00       787\n",
      "                  cold       0.00      0.00      0.00       187\n",
      "         other_weather       0.00      0.00      0.00       452\n",
      "         direct_report       0.00      0.00      0.00      1694\n",
      "\n",
      "             micro avg       0.76      0.24      0.36     27308\n",
      "             macro avg       0.02      0.03      0.02     27308\n",
      "          weighted avg       0.18      0.24      0.21     27308\n",
      "           samples avg       0.76      0.32      0.40     27308\n",
      "\n"
     ]
    }
   ],
   "source": [
    "perf_report(fitted_mdls, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-shops has very little label diversity so it became an edge case, I will drop it for the optimization\n",
    "\n",
    "______________________________ RandomForestClassifier ______________________________\n",
    "\n",
    "\n",
    "                           precision    recall  f1-score   support\n",
    "                           \n",
    "                           \n",
    "             micro avg       0.80      0.44      0.57     27308\n",
    "             \n",
    "             \n",
    "             macro avg       0.58      0.16      0.21     27308\n",
    "             \n",
    "             \n",
    "          weighted avg       0.74      0.44      0.50     27308\n",
    "          \n",
    "          \n",
    "           samples avg       0.65      0.42      0.46     27308\n",
    "           \n",
    "\n",
    "______________________________ ExtraTreesClassifier ______________________________\n",
    "\n",
    "\n",
    "             micro avg       0.79      0.44      0.56     27308\n",
    "             \n",
    "             \n",
    "             macro avg       0.53      0.15      0.21     27308\n",
    "             \n",
    "             \n",
    "          weighted avg       0.71      0.44      0.49     27308\n",
    "          \n",
    "          \n",
    "           samples avg       0.66      0.42      0.46     27308\n",
    "\n",
    "______________________________ GradientBoostingClassifier ______________________________\n",
    "\n",
    "\n",
    "             micro avg       0.76      0.57      0.65     27308\n",
    "             \n",
    "             \n",
    "             macro avg       0.51      0.32      0.38     27308\n",
    "             \n",
    "             \n",
    "          weighted avg       0.72      0.57      0.61     27308\n",
    "          \n",
    "          \n",
    "           samples avg       0.65      0.50      0.52     27308\n",
    "           \n",
    "           \n",
    "______________________________ AdaBoostClassifier ______________________________\n",
    "\n",
    "\n",
    "             micro avg       0.77      0.58      0.66     27308\n",
    "             \n",
    "             \n",
    "             macro avg       0.58      0.33      0.40     27308\n",
    "             \n",
    "             \n",
    "          weighted avg       0.73      0.58      0.62     27308\n",
    "          \n",
    "          \n",
    "           samples avg       0.63      0.50      0.51     27308\n",
    "           \n",
    "______________________________ SVC ______________________________\n",
    "\n",
    "\n",
    "             micro avg       0.76      0.24      0.36     27308\n",
    "             \n",
    "             \n",
    "             macro avg       0.02      0.03      0.02     27308\n",
    "             \n",
    "             \n",
    "          weighted avg       0.18      0.24      0.21     27308\n",
    "          \n",
    "          \n",
    "           samples avg       0.76      0.32      0.40     27308\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve models based on poor target performance elimination\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each.\n",
    "Testing models after dropping poor predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the targets that had the word performances based on the classification report\n",
    "targs_drop = ['offer', 'security', 'infrastructure_related', 'tools', \n",
    "              'hospitals', 'shops', 'aid_centers', 'other_infrastructure', 'fire', 'other_weather']\n",
    "y_min = y.copy()\n",
    "y_min.drop(targs_drop, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_min, random_state = 42, test_size = 0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  RandomForestClassifier\n",
      "{'memory': None, 'steps': [('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=<function tokenize at 0x0000021C73D19F78>,\n",
      "                vocabulary=None)), ('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)), ('clf', MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True,\n",
      "                                                       class_weight=None,\n",
      "                                                       criterion='gini',\n",
      "                                                       max_depth=None,\n",
      "                                                       max_features='auto',\n",
      "                                                       max_leaf_nodes=None,\n",
      "                                                       min_impurity_decrease=0.0,\n",
      "                                                       min_impurity_split=None,\n",
      "                                                       min_samples_leaf=1,\n",
      "                                                       min_samples_split=2,\n",
      "                                                       min_weight_fraction_leaf=0.0,\n",
      "                                                       n_estimators='warn',\n",
      "                                                       n_jobs=None,\n",
      "                                                       oob_score=False,\n",
      "                                                       random_state=None,\n",
      "                                                       verbose=0,\n",
      "                                                       warm_start=False),\n",
      "                      n_jobs=None))], 'verbose': False, 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=<function tokenize at 0x0000021C73D19F78>,\n",
      "                vocabulary=None), 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True), 'clf': MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True,\n",
      "                                                       class_weight=None,\n",
      "                                                       criterion='gini',\n",
      "                                                       max_depth=None,\n",
      "                                                       max_features='auto',\n",
      "                                                       max_leaf_nodes=None,\n",
      "                                                       min_impurity_decrease=0.0,\n",
      "                                                       min_impurity_split=None,\n",
      "                                                       min_samples_leaf=1,\n",
      "                                                       min_samples_split=2,\n",
      "                                                       min_weight_fraction_leaf=0.0,\n",
      "                                                       n_estimators='warn',\n",
      "                                                       n_jobs=None,\n",
      "                                                       oob_score=False,\n",
      "                                                       random_state=None,\n",
      "                                                       verbose=0,\n",
      "                                                       warm_start=False),\n",
      "                      n_jobs=None), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': <function tokenize at 0x0000021C73D19F78>, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'clf__estimator__bootstrap': True, 'clf__estimator__class_weight': None, 'clf__estimator__criterion': 'gini', 'clf__estimator__max_depth': None, 'clf__estimator__max_features': 'auto', 'clf__estimator__max_leaf_nodes': None, 'clf__estimator__min_impurity_decrease': 0.0, 'clf__estimator__min_impurity_split': None, 'clf__estimator__min_samples_leaf': 1, 'clf__estimator__min_samples_split': 2, 'clf__estimator__min_weight_fraction_leaf': 0.0, 'clf__estimator__n_estimators': 'warn', 'clf__estimator__n_jobs': None, 'clf__estimator__oob_score': False, 'clf__estimator__random_state': None, 'clf__estimator__verbose': 0, 'clf__estimator__warm_start': False, 'clf__estimator': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators='warn',\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False), 'clf__n_jobs': None}\n",
      "Model:  ExtraTreesClassifier\n",
      "{'memory': None, 'steps': [('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=<function tokenize at 0x0000021C73D19F78>,\n",
      "                vocabulary=None)), ('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)), ('clf', MultiOutputClassifier(estimator=ExtraTreesClassifier(bootstrap=False,\n",
      "                                                     class_weight=None,\n",
      "                                                     criterion='gini',\n",
      "                                                     max_depth=None,\n",
      "                                                     max_features='auto',\n",
      "                                                     max_leaf_nodes=None,\n",
      "                                                     min_impurity_decrease=0.0,\n",
      "                                                     min_impurity_split=None,\n",
      "                                                     min_samples_leaf=1,\n",
      "                                                     min_samples_split=2,\n",
      "                                                     min_weight_fraction_leaf=0.0,\n",
      "                                                     n_estimators='warn',\n",
      "                                                     n_jobs=None,\n",
      "                                                     oob_score=False,\n",
      "                                                     random_state=None,\n",
      "                                                     verbose=0,\n",
      "                                                     warm_start=False),\n",
      "                      n_jobs=None))], 'verbose': False, 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=<function tokenize at 0x0000021C73D19F78>,\n",
      "                vocabulary=None), 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True), 'clf': MultiOutputClassifier(estimator=ExtraTreesClassifier(bootstrap=False,\n",
      "                                                     class_weight=None,\n",
      "                                                     criterion='gini',\n",
      "                                                     max_depth=None,\n",
      "                                                     max_features='auto',\n",
      "                                                     max_leaf_nodes=None,\n",
      "                                                     min_impurity_decrease=0.0,\n",
      "                                                     min_impurity_split=None,\n",
      "                                                     min_samples_leaf=1,\n",
      "                                                     min_samples_split=2,\n",
      "                                                     min_weight_fraction_leaf=0.0,\n",
      "                                                     n_estimators='warn',\n",
      "                                                     n_jobs=None,\n",
      "                                                     oob_score=False,\n",
      "                                                     random_state=None,\n",
      "                                                     verbose=0,\n",
      "                                                     warm_start=False),\n",
      "                      n_jobs=None), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': <function tokenize at 0x0000021C73D19F78>, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'clf__estimator__bootstrap': False, 'clf__estimator__class_weight': None, 'clf__estimator__criterion': 'gini', 'clf__estimator__max_depth': None, 'clf__estimator__max_features': 'auto', 'clf__estimator__max_leaf_nodes': None, 'clf__estimator__min_impurity_decrease': 0.0, 'clf__estimator__min_impurity_split': None, 'clf__estimator__min_samples_leaf': 1, 'clf__estimator__min_samples_split': 2, 'clf__estimator__min_weight_fraction_leaf': 0.0, 'clf__estimator__n_estimators': 'warn', 'clf__estimator__n_jobs': None, 'clf__estimator__oob_score': False, 'clf__estimator__random_state': None, 'clf__estimator__verbose': 0, 'clf__estimator__warm_start': False, 'clf__estimator': ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "                     max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                     min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                     min_samples_leaf=1, min_samples_split=2,\n",
      "                     min_weight_fraction_leaf=0.0, n_estimators='warn',\n",
      "                     n_jobs=None, oob_score=False, random_state=None, verbose=0,\n",
      "                     warm_start=False), 'clf__n_jobs': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  GradientBoostingClassifier\n",
      "{'memory': None, 'steps': [('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=<function tokenize at 0x0000021C73D19F78>,\n",
      "                vocabulary=None)), ('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)), ('clf', MultiOutputClassifier(estimator=GradientBoostingClassifier(criterion='friedman_mse',\n",
      "                                                           init=None,\n",
      "                                                           learning_rate=0.1,\n",
      "                                                           loss='deviance',\n",
      "                                                           max_depth=3,\n",
      "                                                           max_features=None,\n",
      "                                                           max_leaf_nodes=None,\n",
      "                                                           min_impurity_decrease=0.0,\n",
      "                                                           min_impurity_split=None,\n",
      "                                                           min_samples_leaf=1,\n",
      "                                                           min_samples_split=2,\n",
      "                                                           min_weight_fraction_leaf=0.0,\n",
      "                                                           n_estimators=100,\n",
      "                                                           n_iter_no_change=None,\n",
      "                                                           presort='auto',\n",
      "                                                           random_state=None,\n",
      "                                                           subsample=1.0,\n",
      "                                                           tol=0.0001,\n",
      "                                                           validation_fraction=0.1,\n",
      "                                                           verbose=0,\n",
      "                                                           warm_start=False),\n",
      "                      n_jobs=None))], 'verbose': False, 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=<function tokenize at 0x0000021C73D19F78>,\n",
      "                vocabulary=None), 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True), 'clf': MultiOutputClassifier(estimator=GradientBoostingClassifier(criterion='friedman_mse',\n",
      "                                                           init=None,\n",
      "                                                           learning_rate=0.1,\n",
      "                                                           loss='deviance',\n",
      "                                                           max_depth=3,\n",
      "                                                           max_features=None,\n",
      "                                                           max_leaf_nodes=None,\n",
      "                                                           min_impurity_decrease=0.0,\n",
      "                                                           min_impurity_split=None,\n",
      "                                                           min_samples_leaf=1,\n",
      "                                                           min_samples_split=2,\n",
      "                                                           min_weight_fraction_leaf=0.0,\n",
      "                                                           n_estimators=100,\n",
      "                                                           n_iter_no_change=None,\n",
      "                                                           presort='auto',\n",
      "                                                           random_state=None,\n",
      "                                                           subsample=1.0,\n",
      "                                                           tol=0.0001,\n",
      "                                                           validation_fraction=0.1,\n",
      "                                                           verbose=0,\n",
      "                                                           warm_start=False),\n",
      "                      n_jobs=None), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': <function tokenize at 0x0000021C73D19F78>, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'clf__estimator__criterion': 'friedman_mse', 'clf__estimator__init': None, 'clf__estimator__learning_rate': 0.1, 'clf__estimator__loss': 'deviance', 'clf__estimator__max_depth': 3, 'clf__estimator__max_features': None, 'clf__estimator__max_leaf_nodes': None, 'clf__estimator__min_impurity_decrease': 0.0, 'clf__estimator__min_impurity_split': None, 'clf__estimator__min_samples_leaf': 1, 'clf__estimator__min_samples_split': 2, 'clf__estimator__min_weight_fraction_leaf': 0.0, 'clf__estimator__n_estimators': 100, 'clf__estimator__n_iter_no_change': None, 'clf__estimator__presort': 'auto', 'clf__estimator__random_state': None, 'clf__estimator__subsample': 1.0, 'clf__estimator__tol': 0.0001, 'clf__estimator__validation_fraction': 0.1, 'clf__estimator__verbose': 0, 'clf__estimator__warm_start': False, 'clf__estimator': GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "                           max_features=None, max_leaf_nodes=None,\n",
      "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                           min_samples_leaf=1, min_samples_split=2,\n",
      "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                           n_iter_no_change=None, presort='auto',\n",
      "                           random_state=None, subsample=1.0, tol=0.0001,\n",
      "                           validation_fraction=0.1, verbose=0,\n",
      "                           warm_start=False), 'clf__n_jobs': None}\n",
      "Model:  AdaBoostClassifier\n",
      "{'memory': None, 'steps': [('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=<function tokenize at 0x0000021C73D19F78>,\n",
      "                vocabulary=None)), ('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)), ('clf', MultiOutputClassifier(estimator=AdaBoostClassifier(algorithm='SAMME.R',\n",
      "                                                   base_estimator=None,\n",
      "                                                   learning_rate=1.0,\n",
      "                                                   n_estimators=50,\n",
      "                                                   random_state=None),\n",
      "                      n_jobs=None))], 'verbose': False, 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=<function tokenize at 0x0000021C73D19F78>,\n",
      "                vocabulary=None), 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True), 'clf': MultiOutputClassifier(estimator=AdaBoostClassifier(algorithm='SAMME.R',\n",
      "                                                   base_estimator=None,\n",
      "                                                   learning_rate=1.0,\n",
      "                                                   n_estimators=50,\n",
      "                                                   random_state=None),\n",
      "                      n_jobs=None), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': <function tokenize at 0x0000021C73D19F78>, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'clf__estimator__algorithm': 'SAMME.R', 'clf__estimator__base_estimator': None, 'clf__estimator__learning_rate': 1.0, 'clf__estimator__n_estimators': 50, 'clf__estimator__random_state': None, 'clf__estimator': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
      "                   n_estimators=50, random_state=None), 'clf__n_jobs': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  SVC\n",
      "{'memory': None, 'steps': [('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=<function tokenize at 0x0000021C73D19F78>,\n",
      "                vocabulary=None)), ('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)), ('clf', MultiOutputClassifier(estimator=SVC(C=1.0, cache_size=200, class_weight=None,\n",
      "                                    coef0=0.0, decision_function_shape='ovr',\n",
      "                                    degree=3, gamma='auto_deprecated',\n",
      "                                    kernel='rbf', max_iter=-1,\n",
      "                                    probability=False, random_state=None,\n",
      "                                    shrinking=True, tol=0.001, verbose=False),\n",
      "                      n_jobs=None))], 'verbose': False, 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=<function tokenize at 0x0000021C73D19F78>,\n",
      "                vocabulary=None), 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True), 'clf': MultiOutputClassifier(estimator=SVC(C=1.0, cache_size=200, class_weight=None,\n",
      "                                    coef0=0.0, decision_function_shape='ovr',\n",
      "                                    degree=3, gamma='auto_deprecated',\n",
      "                                    kernel='rbf', max_iter=-1,\n",
      "                                    probability=False, random_state=None,\n",
      "                                    shrinking=True, tol=0.001, verbose=False),\n",
      "                      n_jobs=None), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': <function tokenize at 0x0000021C73D19F78>, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'clf__estimator__C': 1.0, 'clf__estimator__cache_size': 200, 'clf__estimator__class_weight': None, 'clf__estimator__coef0': 0.0, 'clf__estimator__decision_function_shape': 'ovr', 'clf__estimator__degree': 3, 'clf__estimator__gamma': 'auto_deprecated', 'clf__estimator__kernel': 'rbf', 'clf__estimator__max_iter': -1, 'clf__estimator__probability': False, 'clf__estimator__random_state': None, 'clf__estimator__shrinking': True, 'clf__estimator__tol': 0.001, 'clf__estimator__verbose': False, 'clf__estimator': SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "    kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
      "    shrinking=True, tol=0.001, verbose=False), 'clf__n_jobs': None}\n"
     ]
    }
   ],
   "source": [
    "fitted_mdls_min = multi_tester(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = y_train.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________Model______________________________\n",
      "______________________________ RandomForestClassifier ______________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "          related       0.81      0.93      0.87      6534\n",
      "          request       0.83      0.40      0.54      1472\n",
      "      aid_related       0.74      0.52      0.61      3545\n",
      "     medical_help       0.66      0.08      0.14       701\n",
      " medical_products       0.71      0.10      0.17       446\n",
      "search_and_rescue       0.62      0.07      0.12       226\n",
      "         military       0.69      0.07      0.12       267\n",
      "            water       0.82      0.22      0.34       543\n",
      "             food       0.85      0.22      0.35       965\n",
      "          shelter       0.83      0.24      0.37       775\n",
      "         clothing       0.85      0.09      0.16       127\n",
      "            money       0.50      0.01      0.01       191\n",
      "   missing_people       0.00      0.00      0.00       104\n",
      "         refugees       0.54      0.05      0.09       293\n",
      "            death       0.76      0.14      0.24       406\n",
      "        other_aid       0.53      0.03      0.06      1139\n",
      "        transport       0.79      0.08      0.15       407\n",
      "        buildings       0.73      0.09      0.16       441\n",
      "      electricity       0.86      0.03      0.06       185\n",
      "  weather_related       0.84      0.52      0.64      2390\n",
      "           floods       0.87      0.31      0.45       693\n",
      "            storm       0.76      0.36      0.49       812\n",
      "       earthquake       0.87      0.59      0.70       787\n",
      "             cold       0.88      0.07      0.14       187\n",
      "    direct_report       0.78      0.28      0.41      1694\n",
      "\n",
      "        micro avg       0.80      0.48      0.60     25330\n",
      "        macro avg       0.72      0.22      0.30     25330\n",
      "     weighted avg       0.78      0.48      0.53     25330\n",
      "      samples avg       0.66      0.44      0.48     25330\n",
      "\n",
      "______________________________Model______________________________\n",
      "______________________________ ExtraTreesClassifier ______________________________\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "          related       0.82      0.93      0.87      6534\n",
      "          request       0.82      0.36      0.50      1472\n",
      "      aid_related       0.75      0.53      0.62      3545\n",
      "     medical_help       0.57      0.07      0.13       701\n",
      " medical_products       0.50      0.08      0.14       446\n",
      "search_and_rescue       0.43      0.04      0.07       226\n",
      "         military       0.54      0.10      0.17       267\n",
      "            water       0.78      0.15      0.25       543\n",
      "             food       0.80      0.22      0.35       965\n",
      "          shelter       0.82      0.17      0.28       775\n",
      "         clothing       0.67      0.05      0.09       127\n",
      "            money       0.56      0.05      0.09       191\n",
      "   missing_people       0.80      0.04      0.07       104\n",
      "         refugees       0.44      0.03      0.05       293\n",
      "            death       0.84      0.08      0.14       406\n",
      "        other_aid       0.51      0.04      0.07      1139\n",
      "        transport       0.48      0.05      0.09       407\n",
      "        buildings       0.56      0.08      0.14       441\n",
      "      electricity       0.73      0.04      0.08       185\n",
      "  weather_related       0.81      0.53      0.64      2390\n",
      "           floods       0.85      0.22      0.35       693\n",
      "            storm       0.72      0.29      0.42       812\n",
      "       earthquake       0.88      0.43      0.58       787\n",
      "             cold       0.62      0.10      0.17       187\n",
      "    direct_report       0.78      0.29      0.42      1694\n",
      "\n",
      "        micro avg       0.79      0.46      0.59     25330\n",
      "        macro avg       0.68      0.20      0.27     25330\n",
      "     weighted avg       0.75      0.46      0.52     25330\n",
      "      samples avg       0.65      0.43      0.47     25330\n",
      "\n",
      "______________________________Model______________________________\n",
      "______________________________ GradientBoostingClassifier ______________________________\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "          related       0.80      0.97      0.88      6534\n",
      "          request       0.84      0.52      0.65      1472\n",
      "      aid_related       0.77      0.57      0.65      3545\n",
      "     medical_help       0.65      0.23      0.33       701\n",
      " medical_products       0.64      0.28      0.39       446\n",
      "search_and_rescue       0.29      0.19      0.23       226\n",
      "         military       0.50      0.22      0.30       267\n",
      "            water       0.74      0.66      0.70       543\n",
      "             food       0.80      0.78      0.79       965\n",
      "          shelter       0.80      0.54      0.64       775\n",
      "         clothing       0.56      0.46      0.50       127\n",
      "            money       0.49      0.25      0.33       191\n",
      "   missing_people       0.29      0.26      0.27       104\n",
      "         refugees       0.47      0.26      0.33       293\n",
      "            death       0.70      0.49      0.58       406\n",
      "        other_aid       0.60      0.09      0.15      1139\n",
      "        transport       0.54      0.24      0.33       407\n",
      "        buildings       0.69      0.29      0.41       441\n",
      "      electricity       0.40      0.22      0.28       185\n",
      "  weather_related       0.87      0.59      0.70      2390\n",
      "           floods       0.88      0.53      0.66       693\n",
      "            storm       0.76      0.59      0.67       812\n",
      "       earthquake       0.83      0.79      0.81       787\n",
      "             cold       0.48      0.35      0.40       187\n",
      "    direct_report       0.79      0.43      0.56      1694\n",
      "\n",
      "        micro avg       0.78      0.61      0.68     25330\n",
      "        macro avg       0.65      0.43      0.50     25330\n",
      "     weighted avg       0.76      0.61      0.65     25330\n",
      "      samples avg       0.66      0.52      0.54     25330\n",
      "\n",
      "______________________________Model______________________________\n",
      "______________________________ AdaBoostClassifier ______________________________\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "          related       0.81      0.94      0.87      6534\n",
      "          request       0.78      0.54      0.64      1472\n",
      "      aid_related       0.75      0.60      0.66      3545\n",
      "     medical_help       0.57      0.26      0.36       701\n",
      " medical_products       0.63      0.30      0.41       446\n",
      "search_and_rescue       0.55      0.17      0.26       226\n",
      "         military       0.53      0.28      0.37       267\n",
      "            water       0.72      0.61      0.66       543\n",
      "             food       0.80      0.68      0.74       965\n",
      "          shelter       0.76      0.57      0.65       775\n",
      "         clothing       0.65      0.40      0.50       127\n",
      "            money       0.49      0.28      0.35       191\n",
      "   missing_people       0.65      0.11      0.18       104\n",
      "         refugees       0.60      0.29      0.39       293\n",
      "            death       0.78      0.42      0.54       406\n",
      "        other_aid       0.55      0.16      0.25      1139\n",
      "        transport       0.64      0.16      0.26       407\n",
      "        buildings       0.69      0.37      0.48       441\n",
      "      electricity       0.51      0.19      0.28       185\n",
      "  weather_related       0.85      0.65      0.74      2390\n",
      "           floods       0.86      0.56      0.68       693\n",
      "            storm       0.73      0.50      0.60       812\n",
      "       earthquake       0.87      0.75      0.80       787\n",
      "             cold       0.74      0.34      0.47       187\n",
      "    direct_report       0.71      0.47      0.57      1694\n",
      "\n",
      "        micro avg       0.77      0.61      0.69     25330\n",
      "        macro avg       0.69      0.42      0.51     25330\n",
      "     weighted avg       0.75      0.61      0.66     25330\n",
      "      samples avg       0.64      0.51      0.53     25330\n",
      "\n",
      "______________________________Model______________________________\n",
      "______________________________ SVC ______________________________\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "          related       0.76      1.00      0.86      6534\n",
      "          request       0.00      0.00      0.00      1472\n",
      "      aid_related       0.00      0.00      0.00      3545\n",
      "     medical_help       0.00      0.00      0.00       701\n",
      " medical_products       0.00      0.00      0.00       446\n",
      "search_and_rescue       0.00      0.00      0.00       226\n",
      "         military       0.00      0.00      0.00       267\n",
      "            water       0.00      0.00      0.00       543\n",
      "             food       0.00      0.00      0.00       965\n",
      "          shelter       0.00      0.00      0.00       775\n",
      "         clothing       0.00      0.00      0.00       127\n",
      "            money       0.00      0.00      0.00       191\n",
      "   missing_people       0.00      0.00      0.00       104\n",
      "         refugees       0.00      0.00      0.00       293\n",
      "            death       0.00      0.00      0.00       406\n",
      "        other_aid       0.00      0.00      0.00      1139\n",
      "        transport       0.00      0.00      0.00       407\n",
      "        buildings       0.00      0.00      0.00       441\n",
      "      electricity       0.00      0.00      0.00       185\n",
      "  weather_related       0.00      0.00      0.00      2390\n",
      "           floods       0.00      0.00      0.00       693\n",
      "            storm       0.00      0.00      0.00       812\n",
      "       earthquake       0.00      0.00      0.00       787\n",
      "             cold       0.00      0.00      0.00       187\n",
      "    direct_report       0.00      0.00      0.00      1694\n",
      "\n",
      "        micro avg       0.76      0.26      0.38     25330\n",
      "        macro avg       0.03      0.04      0.03     25330\n",
      "     weighted avg       0.19      0.26      0.22     25330\n",
      "      samples avg       0.76      0.33      0.41     25330\n",
      "\n"
     ]
    }
   ],
   "source": [
    "perf_report(fitted_mdls_min, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "______________________________ RandomForestClassifier ______________________________\n",
    "\n",
    "                           precision    recall  f1-score   support \n",
    "                           \n",
    "              micro avg       0.80      0.48      0.60     25330\n",
    "              \n",
    "              macro avg       0.72      0.22      0.30     25330\n",
    "              \n",
    "           weighted avg       0.78      0.48      0.53     25330\n",
    "           \n",
    "            samples avg       0.66      0.44      0.48     25330\n",
    "\n",
    "______________________________ ExtraTreesClassifier ______________________________\n",
    "\n",
    "        micro avg       0.79      0.46      0.59     25330\n",
    "        \n",
    "        macro avg       0.68      0.20      0.27     25330\n",
    "        \n",
    "     weighted avg       0.75      0.46      0.52     25330\n",
    "     \n",
    "      samples avg       0.65      0.43      0.47     25330    \n",
    "\n",
    "______________________________ GradientBoostingClassifier ______________________________\n",
    "\n",
    "        micro avg       0.78      0.61      0.68     25330\n",
    "        \n",
    "        macro avg       0.65      0.43      0.50     25330\n",
    "        \n",
    "     weighted avg       0.76      0.61      0.65     25330\n",
    "     \n",
    "      samples avg       0.66      0.52      0.54     25330\n",
    "           \n",
    "           \n",
    "______________________________ AdaBoostClassifier ______________________________\n",
    "\n",
    "        micro avg       0.77      0.61      0.69     25330\n",
    "        \n",
    "        macro avg       0.69      0.42      0.51     25330\n",
    "        \n",
    "     weighted avg       0.75      0.61      0.66     25330\n",
    "     \n",
    "      samples avg       0.64      0.51      0.53     25330\n",
    "           \n",
    "______________________________ SVC ______________________________\n",
    "\n",
    "        micro avg       0.76      0.26      0.38     25330\n",
    "        \n",
    "        macro avg       0.03      0.04      0.03     25330\n",
    "        \n",
    "     weighted avg       0.19      0.26      0.22     25330\n",
    "     \n",
    "      samples avg       0.76      0.33      0.41     25330\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Improve your model\n",
    "Use grid search to find better parameters. \n",
    "\n",
    "I will work on my best performing model adaboost and using the reduced target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(AdaBoostClassifier()))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('vect',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function tokenize at 0x0000021C73D19F78>,\n",
       "                   vocabulary=None)),\n",
       "  ('tfidf',\n",
       "   TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),\n",
       "  ('clf',\n",
       "   MultiOutputClassifier(estimator=AdaBoostClassifier(algorithm='SAMME.R',\n",
       "                                                      base_estimator=None,\n",
       "                                                      learning_rate=1.0,\n",
       "                                                      n_estimators=50,\n",
       "                                                      random_state=None),\n",
       "                         n_jobs=None))],\n",
       " 'verbose': False,\n",
       " 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function tokenize at 0x0000021C73D19F78>,\n",
       "                 vocabulary=None),\n",
       " 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
       " 'clf': MultiOutputClassifier(estimator=AdaBoostClassifier(algorithm='SAMME.R',\n",
       "                                                    base_estimator=None,\n",
       "                                                    learning_rate=1.0,\n",
       "                                                    n_estimators=50,\n",
       "                                                    random_state=None),\n",
       "                       n_jobs=None),\n",
       " 'vect__analyzer': 'word',\n",
       " 'vect__binary': False,\n",
       " 'vect__decode_error': 'strict',\n",
       " 'vect__dtype': numpy.int64,\n",
       " 'vect__encoding': 'utf-8',\n",
       " 'vect__input': 'content',\n",
       " 'vect__lowercase': True,\n",
       " 'vect__max_df': 1.0,\n",
       " 'vect__max_features': None,\n",
       " 'vect__min_df': 1,\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__preprocessor': None,\n",
       " 'vect__stop_words': None,\n",
       " 'vect__strip_accents': None,\n",
       " 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vect__tokenizer': <function __main__.tokenize(text)>,\n",
       " 'vect__vocabulary': None,\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__smooth_idf': True,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__use_idf': True,\n",
       " 'clf__estimator__algorithm': 'SAMME.R',\n",
       " 'clf__estimator__base_estimator': None,\n",
       " 'clf__estimator__learning_rate': 1.0,\n",
       " 'clf__estimator__n_estimators': 50,\n",
       " 'clf__estimator__random_state': None,\n",
       " 'clf__estimator': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       " 'clf__n_jobs': None}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'tfidf__use_idf': (True, False),\n",
    "              'clf__estimator__n_estimators': [50, 300], \n",
    "              'clf__estimator__random_state': [42],\n",
    "             'clf__estimator__learning_rate': [0.2]} \n",
    "\n",
    "cv = GridSearchCV(pipeline, param_grid = parameters, cv = 10,\n",
    "                  refit = True, verbose = 1, return_train_score = True, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('vect',\n",
       "                                        CountVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.int64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        preprocessor=None,\n",
       "                                                        stop_words=None,\n",
       "                                                        strip_accents=None,\n",
       "                                                        to...\n",
       "                                                                                           base_estimator=None,\n",
       "                                                                                           learning_rate=1.0,\n",
       "                                                                                           n_estimators=50,\n",
       "                                                                                           random_state=None),\n",
       "                                                              n_jobs=None))],\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'clf__estimator__learning_rate': [0.2],\n",
       "                         'clf__estimator__n_estimators': [50, 300],\n",
       "                         'clf__estimator__random_state': [42],\n",
       "                         'tfidf__use_idf': (True, False)},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "             scoring=None, verbose=1)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "cv.fit(X_train, y_train)\n",
    "perf_report(cv, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Other Approaches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_xg = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "    ('best', TruncatedSVD()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', XGBClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_xg.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_xg = {'nthread':[4], \n",
    "              'objective':['binary:logistic'],\n",
    "              'learning_rate': [0.05], \n",
    "              'max_depth': [6],\n",
    "              'min_child_weight': [11],\n",
    "              'silent': [1],\n",
    "              'subsample': [0.8],\n",
    "              'colsample_bytree': [0.7],\n",
    "              'n_estimators': [500], \n",
    "              'seed': [42]}\n",
    "\n",
    "cv_xg = GridSearchCV(pipeline_xg, param_grid = parameters_xg, n_jobs = -1, n_folds=5), \n",
    "                   scoring='roc_auc', verbose=2, refit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_xg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_xg.fit(X_train, y_train)\n",
    "perf_report(cv_xg, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
