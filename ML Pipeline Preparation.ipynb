{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sousa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sousa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.base import BaseEstimator\n",
    "import pickle\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "nltk.download(['punkt', 'wordnet'])\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "#imblearn\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///messages.db')\n",
    "df = pd.read_sql_table(\"messages\", con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>message</th>\n",
       "      <th>original</th>\n",
       "      <th>genre</th>\n",
       "      <th>related</th>\n",
       "      <th>request</th>\n",
       "      <th>offer</th>\n",
       "      <th>aid_related</th>\n",
       "      <th>medical_help</th>\n",
       "      <th>medical_products</th>\n",
       "      <th>...</th>\n",
       "      <th>aid_centers</th>\n",
       "      <th>other_infrastructure</th>\n",
       "      <th>weather_related</th>\n",
       "      <th>floods</th>\n",
       "      <th>storm</th>\n",
       "      <th>fire</th>\n",
       "      <th>earthquake</th>\n",
       "      <th>cold</th>\n",
       "      <th>other_weather</th>\n",
       "      <th>direct_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Weather update - a cold front from Cuba that c...</td>\n",
       "      <td>Un front froid se retrouve sur Cuba ce matin. ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>Is the Hurricane over or is it not over</td>\n",
       "      <td>Cyclone nan fini osinon li pa fini</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>Looking for someone but no name</td>\n",
       "      <td>Patnm, di Maryani relem pou li banm nouvel li ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>says: west side of Haiti, rest of the country ...</td>\n",
       "      <td>facade ouest d Haiti et le reste du pays aujou...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            message  \\\n",
       "0   2  Weather update - a cold front from Cuba that c...   \n",
       "1   7            Is the Hurricane over or is it not over   \n",
       "2   8                    Looking for someone but no name   \n",
       "3   9  UN reports Leogane 80-90 destroyed. Only Hospi...   \n",
       "4  12  says: west side of Haiti, rest of the country ...   \n",
       "\n",
       "                                            original   genre  related  \\\n",
       "0  Un front froid se retrouve sur Cuba ce matin. ...  direct        1   \n",
       "1                 Cyclone nan fini osinon li pa fini  direct        1   \n",
       "2  Patnm, di Maryani relem pou li banm nouvel li ...  direct        1   \n",
       "3  UN reports Leogane 80-90 destroyed. Only Hospi...  direct        1   \n",
       "4  facade ouest d Haiti et le reste du pays aujou...  direct        1   \n",
       "\n",
       "   request  offer  aid_related  medical_help  medical_products  ...  \\\n",
       "0        0      0            0             0                 0  ...   \n",
       "1        0      0            1             0                 0  ...   \n",
       "2        0      0            0             0                 0  ...   \n",
       "3        1      0            1             0                 1  ...   \n",
       "4        0      0            0             0                 0  ...   \n",
       "\n",
       "   aid_centers  other_infrastructure  weather_related  floods  storm  fire  \\\n",
       "0            0                     0                0       0      0     0   \n",
       "1            0                     0                1       0      1     0   \n",
       "2            0                     0                0       0      0     0   \n",
       "3            0                     0                0       0      0     0   \n",
       "4            0                     0                0       0      0     0   \n",
       "\n",
       "   earthquake  cold  other_weather  direct_report  \n",
       "0           0     0              0              0  \n",
       "1           0     0              0              0  \n",
       "2           0     0              0              0  \n",
       "3           0     0              0              0  \n",
       "4           0     0              0              0  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on this quick check most of the data is very imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"message\"]\n",
    "y = df.drop(['message', 'genre', 'id', 'original'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    '''\n",
    "    Receives text related data and processes it\n",
    "    Args: text related data (columns)\n",
    "    Returns: tokenized text\n",
    "    '''\n",
    "    # get list of all urls using regex\n",
    "    detected_urls = re.findall(url_regex, text) \n",
    "    \n",
    "    # replace each url in text string with placeholder\n",
    "    for url in detected_urls:\n",
    "        text = text.replace(url, \"urlplaceholder\")\n",
    "\n",
    "    # tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # initiate lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # iterate through each token\n",
    "    clean_tokens = []\n",
    "    for tok in tokens:\n",
    "        \n",
    "        # lemmatize, normalize case, and remove leading/trailing white space\n",
    "        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n",
    "        clean_tokens.append(clean_tok)\n",
    "\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_tester(X, y):\n",
    "    '''\n",
    "    Function to create list of fitted models\n",
    "    Args: training data X and y\n",
    "    returns: list of the selected fitted models\n",
    "    '''\n",
    "    pipe_1 = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "    ])\n",
    "    \n",
    "    pipe_2 = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', MultiOutputClassifier(ExtraTreesClassifier()))\n",
    "    ])\n",
    "    \n",
    "    pipe_3 = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', MultiOutputClassifier(GradientBoostingClassifier()))\n",
    "    ])\n",
    "    \n",
    "    pipe_4 = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', MultiOutputClassifier(AdaBoostClassifier()))\n",
    "    ])\n",
    "    \n",
    "    pipe_5 = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', MultiOutputClassifier(SVC()))\n",
    "    ])\n",
    "    \n",
    "    pips = [pipe_1, pipe_2, pipe_3, pipe_4, pipe_5]\n",
    "    pip_names = ['RandomForestClassifier', 'ExtraTreesClassifier', 'GradientBoostingClassifier', \n",
    "                 'AdaBoostClassifier', 'SVC']\n",
    "    \n",
    "    model_fits = []\n",
    "    for i in range(len(pips)):\n",
    "        print('Model: ', pip_names[i])\n",
    "        print(pips[i].get_params())\n",
    "        mdl = pips[i].fit(X, y)\n",
    "        model_fits.append(mdl)\n",
    "        \n",
    "    return model_fits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size = 0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  RandomForestClassifier\n",
      "{'memory': None, 'steps': [('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=<function tokenize at 0x0000028973392D38>,\n",
      "                vocabulary=None)), ('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)), ('clf', MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True,\n",
      "                                                       class_weight=None,\n",
      "                                                       criterion='gini',\n",
      "                                                       max_depth=None,\n",
      "                                                       max_features='auto',\n",
      "                                                       max_leaf_nodes=None,\n",
      "                                                       min_impurity_decrease=0.0,\n",
      "                                                       min_impurity_split=None,\n",
      "                                                       min_samples_leaf=1,\n",
      "                                                       min_samples_split=2,\n",
      "                                                       min_weight_fraction_leaf=0.0,\n",
      "                                                       n_estimators='warn',\n",
      "                                                       n_jobs=None,\n",
      "                                                       oob_score=False,\n",
      "                                                       random_state=None,\n",
      "                                                       verbose=0,\n",
      "                                                       warm_start=False),\n",
      "                      n_jobs=None))], 'verbose': False, 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=<function tokenize at 0x0000028973392D38>,\n",
      "                vocabulary=None), 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True), 'clf': MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True,\n",
      "                                                       class_weight=None,\n",
      "                                                       criterion='gini',\n",
      "                                                       max_depth=None,\n",
      "                                                       max_features='auto',\n",
      "                                                       max_leaf_nodes=None,\n",
      "                                                       min_impurity_decrease=0.0,\n",
      "                                                       min_impurity_split=None,\n",
      "                                                       min_samples_leaf=1,\n",
      "                                                       min_samples_split=2,\n",
      "                                                       min_weight_fraction_leaf=0.0,\n",
      "                                                       n_estimators='warn',\n",
      "                                                       n_jobs=None,\n",
      "                                                       oob_score=False,\n",
      "                                                       random_state=None,\n",
      "                                                       verbose=0,\n",
      "                                                       warm_start=False),\n",
      "                      n_jobs=None), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': <function tokenize at 0x0000028973392D38>, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'clf__estimator__bootstrap': True, 'clf__estimator__class_weight': None, 'clf__estimator__criterion': 'gini', 'clf__estimator__max_depth': None, 'clf__estimator__max_features': 'auto', 'clf__estimator__max_leaf_nodes': None, 'clf__estimator__min_impurity_decrease': 0.0, 'clf__estimator__min_impurity_split': None, 'clf__estimator__min_samples_leaf': 1, 'clf__estimator__min_samples_split': 2, 'clf__estimator__min_weight_fraction_leaf': 0.0, 'clf__estimator__n_estimators': 'warn', 'clf__estimator__n_jobs': None, 'clf__estimator__oob_score': False, 'clf__estimator__random_state': None, 'clf__estimator__verbose': 0, 'clf__estimator__warm_start': False, 'clf__estimator': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators='warn',\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False), 'clf__n_jobs': None}\n",
      "Model:  ExtraTreesClassifier\n",
      "{'memory': None, 'steps': [('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=<function tokenize at 0x0000028973392D38>,\n",
      "                vocabulary=None)), ('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)), ('clf', MultiOutputClassifier(estimator=ExtraTreesClassifier(bootstrap=False,\n",
      "                                                     class_weight=None,\n",
      "                                                     criterion='gini',\n",
      "                                                     max_depth=None,\n",
      "                                                     max_features='auto',\n",
      "                                                     max_leaf_nodes=None,\n",
      "                                                     min_impurity_decrease=0.0,\n",
      "                                                     min_impurity_split=None,\n",
      "                                                     min_samples_leaf=1,\n",
      "                                                     min_samples_split=2,\n",
      "                                                     min_weight_fraction_leaf=0.0,\n",
      "                                                     n_estimators='warn',\n",
      "                                                     n_jobs=None,\n",
      "                                                     oob_score=False,\n",
      "                                                     random_state=None,\n",
      "                                                     verbose=0,\n",
      "                                                     warm_start=False),\n",
      "                      n_jobs=None))], 'verbose': False, 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=<function tokenize at 0x0000028973392D38>,\n",
      "                vocabulary=None), 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True), 'clf': MultiOutputClassifier(estimator=ExtraTreesClassifier(bootstrap=False,\n",
      "                                                     class_weight=None,\n",
      "                                                     criterion='gini',\n",
      "                                                     max_depth=None,\n",
      "                                                     max_features='auto',\n",
      "                                                     max_leaf_nodes=None,\n",
      "                                                     min_impurity_decrease=0.0,\n",
      "                                                     min_impurity_split=None,\n",
      "                                                     min_samples_leaf=1,\n",
      "                                                     min_samples_split=2,\n",
      "                                                     min_weight_fraction_leaf=0.0,\n",
      "                                                     n_estimators='warn',\n",
      "                                                     n_jobs=None,\n",
      "                                                     oob_score=False,\n",
      "                                                     random_state=None,\n",
      "                                                     verbose=0,\n",
      "                                                     warm_start=False),\n",
      "                      n_jobs=None), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': <function tokenize at 0x0000028973392D38>, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'clf__estimator__bootstrap': False, 'clf__estimator__class_weight': None, 'clf__estimator__criterion': 'gini', 'clf__estimator__max_depth': None, 'clf__estimator__max_features': 'auto', 'clf__estimator__max_leaf_nodes': None, 'clf__estimator__min_impurity_decrease': 0.0, 'clf__estimator__min_impurity_split': None, 'clf__estimator__min_samples_leaf': 1, 'clf__estimator__min_samples_split': 2, 'clf__estimator__min_weight_fraction_leaf': 0.0, 'clf__estimator__n_estimators': 'warn', 'clf__estimator__n_jobs': None, 'clf__estimator__oob_score': False, 'clf__estimator__random_state': None, 'clf__estimator__verbose': 0, 'clf__estimator__warm_start': False, 'clf__estimator': ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "                     max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                     min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                     min_samples_leaf=1, min_samples_split=2,\n",
      "                     min_weight_fraction_leaf=0.0, n_estimators='warn',\n",
      "                     n_jobs=None, oob_score=False, random_state=None, verbose=0,\n",
      "                     warm_start=False), 'clf__n_jobs': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  GradientBoostingClassifier\n",
      "{'memory': None, 'steps': [('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=<function tokenize at 0x0000028973392D38>,\n",
      "                vocabulary=None)), ('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)), ('clf', MultiOutputClassifier(estimator=GradientBoostingClassifier(criterion='friedman_mse',\n",
      "                                                           init=None,\n",
      "                                                           learning_rate=0.1,\n",
      "                                                           loss='deviance',\n",
      "                                                           max_depth=3,\n",
      "                                                           max_features=None,\n",
      "                                                           max_leaf_nodes=None,\n",
      "                                                           min_impurity_decrease=0.0,\n",
      "                                                           min_impurity_split=None,\n",
      "                                                           min_samples_leaf=1,\n",
      "                                                           min_samples_split=2,\n",
      "                                                           min_weight_fraction_leaf=0.0,\n",
      "                                                           n_estimators=100,\n",
      "                                                           n_iter_no_change=None,\n",
      "                                                           presort='auto',\n",
      "                                                           random_state=None,\n",
      "                                                           subsample=1.0,\n",
      "                                                           tol=0.0001,\n",
      "                                                           validation_fraction=0.1,\n",
      "                                                           verbose=0,\n",
      "                                                           warm_start=False),\n",
      "                      n_jobs=None))], 'verbose': False, 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=<function tokenize at 0x0000028973392D38>,\n",
      "                vocabulary=None), 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True), 'clf': MultiOutputClassifier(estimator=GradientBoostingClassifier(criterion='friedman_mse',\n",
      "                                                           init=None,\n",
      "                                                           learning_rate=0.1,\n",
      "                                                           loss='deviance',\n",
      "                                                           max_depth=3,\n",
      "                                                           max_features=None,\n",
      "                                                           max_leaf_nodes=None,\n",
      "                                                           min_impurity_decrease=0.0,\n",
      "                                                           min_impurity_split=None,\n",
      "                                                           min_samples_leaf=1,\n",
      "                                                           min_samples_split=2,\n",
      "                                                           min_weight_fraction_leaf=0.0,\n",
      "                                                           n_estimators=100,\n",
      "                                                           n_iter_no_change=None,\n",
      "                                                           presort='auto',\n",
      "                                                           random_state=None,\n",
      "                                                           subsample=1.0,\n",
      "                                                           tol=0.0001,\n",
      "                                                           validation_fraction=0.1,\n",
      "                                                           verbose=0,\n",
      "                                                           warm_start=False),\n",
      "                      n_jobs=None), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': <function tokenize at 0x0000028973392D38>, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'clf__estimator__criterion': 'friedman_mse', 'clf__estimator__init': None, 'clf__estimator__learning_rate': 0.1, 'clf__estimator__loss': 'deviance', 'clf__estimator__max_depth': 3, 'clf__estimator__max_features': None, 'clf__estimator__max_leaf_nodes': None, 'clf__estimator__min_impurity_decrease': 0.0, 'clf__estimator__min_impurity_split': None, 'clf__estimator__min_samples_leaf': 1, 'clf__estimator__min_samples_split': 2, 'clf__estimator__min_weight_fraction_leaf': 0.0, 'clf__estimator__n_estimators': 100, 'clf__estimator__n_iter_no_change': None, 'clf__estimator__presort': 'auto', 'clf__estimator__random_state': None, 'clf__estimator__subsample': 1.0, 'clf__estimator__tol': 0.0001, 'clf__estimator__validation_fraction': 0.1, 'clf__estimator__verbose': 0, 'clf__estimator__warm_start': False, 'clf__estimator': GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "                           max_features=None, max_leaf_nodes=None,\n",
      "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                           min_samples_leaf=1, min_samples_split=2,\n",
      "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                           n_iter_no_change=None, presort='auto',\n",
      "                           random_state=None, subsample=1.0, tol=0.0001,\n",
      "                           validation_fraction=0.1, verbose=0,\n",
      "                           warm_start=False), 'clf__n_jobs': None}\n",
      "Model:  AdaBoostClassifier\n",
      "{'memory': None, 'steps': [('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=<function tokenize at 0x0000028973392D38>,\n",
      "                vocabulary=None)), ('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)), ('clf', MultiOutputClassifier(estimator=AdaBoostClassifier(algorithm='SAMME.R',\n",
      "                                                   base_estimator=None,\n",
      "                                                   learning_rate=1.0,\n",
      "                                                   n_estimators=50,\n",
      "                                                   random_state=None),\n",
      "                      n_jobs=None))], 'verbose': False, 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=<function tokenize at 0x0000028973392D38>,\n",
      "                vocabulary=None), 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True), 'clf': MultiOutputClassifier(estimator=AdaBoostClassifier(algorithm='SAMME.R',\n",
      "                                                   base_estimator=None,\n",
      "                                                   learning_rate=1.0,\n",
      "                                                   n_estimators=50,\n",
      "                                                   random_state=None),\n",
      "                      n_jobs=None), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': <function tokenize at 0x0000028973392D38>, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'clf__estimator__algorithm': 'SAMME.R', 'clf__estimator__base_estimator': None, 'clf__estimator__learning_rate': 1.0, 'clf__estimator__n_estimators': 50, 'clf__estimator__random_state': None, 'clf__estimator': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
      "                   n_estimators=50, random_state=None), 'clf__n_jobs': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  SVC\n",
      "{'memory': None, 'steps': [('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=<function tokenize at 0x0000028973392D38>,\n",
      "                vocabulary=None)), ('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)), ('clf', MultiOutputClassifier(estimator=SVC(C=1.0, cache_size=200, class_weight=None,\n",
      "                                    coef0=0.0, decision_function_shape='ovr',\n",
      "                                    degree=3, gamma='auto_deprecated',\n",
      "                                    kernel='rbf', max_iter=-1,\n",
      "                                    probability=False, random_state=None,\n",
      "                                    shrinking=True, tol=0.001, verbose=False),\n",
      "                      n_jobs=None))], 'verbose': False, 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=<function tokenize at 0x0000028973392D38>,\n",
      "                vocabulary=None), 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True), 'clf': MultiOutputClassifier(estimator=SVC(C=1.0, cache_size=200, class_weight=None,\n",
      "                                    coef0=0.0, decision_function_shape='ovr',\n",
      "                                    degree=3, gamma='auto_deprecated',\n",
      "                                    kernel='rbf', max_iter=-1,\n",
      "                                    probability=False, random_state=None,\n",
      "                                    shrinking=True, tol=0.001, verbose=False),\n",
      "                      n_jobs=None), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': <function tokenize at 0x0000028973392D38>, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'clf__estimator__C': 1.0, 'clf__estimator__cache_size': 200, 'clf__estimator__class_weight': None, 'clf__estimator__coef0': 0.0, 'clf__estimator__decision_function_shape': 'ovr', 'clf__estimator__degree': 3, 'clf__estimator__gamma': 'auto_deprecated', 'clf__estimator__kernel': 'rbf', 'clf__estimator__max_iter': -1, 'clf__estimator__probability': False, 'clf__estimator__random_state': None, 'clf__estimator__shrinking': True, 'clf__estimator__tol': 0.001, 'clf__estimator__verbose': False, 'clf__estimator': SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "    kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
      "    shrinking=True, tol=0.001, verbose=False), 'clf__n_jobs': None}\n"
     ]
    }
   ],
   "source": [
    "fitted_mdls = multi_tester(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your models\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = y_train.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perf_report(model, X_test, y_test):\n",
    "    '''\n",
    "    Function to return model classification reports\n",
    "    Input: Model list, and test data \n",
    "    Output: Prints the Classification report\n",
    "    '''\n",
    "    pip_names = ['RandomForestClassifier', 'ExtraTreesClassifier', 'GradientBoostingClassifier', \n",
    "             'AdaBoostClassifier', 'SVC']\n",
    "    \n",
    "    for i in range(len(model)):\n",
    "        print('______________________________Model______________________________')\n",
    "        print('______________________________', pip_names[i], '______________________________')\n",
    "        y_pred = model[i].predict(X_test)\n",
    "        print(classification_report(y_test, y_pred, target_names = target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________Model______________________________\n",
      "______________________________ RandomForestClassifier ______________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related       0.81      0.92      0.86      6534\n",
      "               request       0.83      0.34      0.48      1472\n",
      "                 offer       0.00      0.00      0.00        38\n",
      "           aid_related       0.74      0.53      0.62      3545\n",
      "          medical_help       0.64      0.09      0.15       701\n",
      "      medical_products       0.69      0.07      0.13       446\n",
      "     search_and_rescue       0.74      0.08      0.14       226\n",
      "              security       0.00      0.00      0.00       160\n",
      "              military       0.65      0.08      0.15       267\n",
      "                 water       0.82      0.27      0.41       543\n",
      "                  food       0.83      0.39      0.53       965\n",
      "               shelter       0.84      0.20      0.33       775\n",
      "              clothing       0.75      0.02      0.05       127\n",
      "                 money       0.50      0.02      0.04       191\n",
      "        missing_people       1.00      0.01      0.02       104\n",
      "              refugees       0.40      0.01      0.01       293\n",
      "                 death       0.82      0.09      0.16       406\n",
      "             other_aid       0.51      0.03      0.05      1139\n",
      "infrastructure_related       0.12      0.00      0.00       568\n",
      "             transport       0.76      0.07      0.13       407\n",
      "             buildings       0.71      0.08      0.15       441\n",
      "           electricity       0.75      0.03      0.06       185\n",
      "                 tools       0.00      0.00      0.00        53\n",
      "             hospitals       0.00      0.00      0.00        85\n",
      "                 shops       0.00      0.00      0.00        34\n",
      "           aid_centers       0.00      0.00      0.00       115\n",
      "  other_infrastructure       0.50      0.00      0.01       383\n",
      "       weather_related       0.83      0.50      0.63      2390\n",
      "                floods       0.90      0.32      0.48       693\n",
      "                 storm       0.77      0.31      0.44       812\n",
      "                  fire       0.50      0.01      0.02        90\n",
      "            earthquake       0.88      0.71      0.79       787\n",
      "                  cold       0.65      0.09      0.16       187\n",
      "         other_weather       0.60      0.03      0.06       452\n",
      "         direct_report       0.78      0.29      0.42      1694\n",
      "\n",
      "             micro avg       0.80      0.44      0.57     27308\n",
      "             macro avg       0.58      0.16      0.21     27308\n",
      "          weighted avg       0.74      0.44      0.50     27308\n",
      "           samples avg       0.65      0.42      0.46     27308\n",
      "\n",
      "______________________________Model______________________________\n",
      "______________________________ ExtraTreesClassifier ______________________________\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related       0.82      0.93      0.87      6534\n",
      "               request       0.83      0.36      0.50      1472\n",
      "                 offer       0.00      0.00      0.00        38\n",
      "           aid_related       0.75      0.53      0.62      3545\n",
      "          medical_help       0.51      0.08      0.13       701\n",
      "      medical_products       0.52      0.07      0.13       446\n",
      "     search_and_rescue       0.37      0.03      0.06       226\n",
      "              security       0.00      0.00      0.00       160\n",
      "              military       0.56      0.07      0.12       267\n",
      "                 water       0.72      0.15      0.25       543\n",
      "                  food       0.84      0.24      0.37       965\n",
      "               shelter       0.81      0.20      0.32       775\n",
      "              clothing       0.80      0.06      0.12       127\n",
      "                 money       0.38      0.03      0.06       191\n",
      "        missing_people       0.40      0.02      0.04       104\n",
      "              refugees       0.40      0.03      0.06       293\n",
      "                 death       0.77      0.10      0.18       406\n",
      "             other_aid       0.43      0.03      0.06      1139\n",
      "infrastructure_related       0.23      0.01      0.01       568\n",
      "             transport       0.41      0.05      0.10       407\n",
      "             buildings       0.67      0.09      0.16       441\n",
      "           electricity       0.53      0.04      0.08       185\n",
      "                 tools       0.00      0.00      0.00        53\n",
      "             hospitals       0.00      0.00      0.00        85\n",
      "                 shops       0.00      0.00      0.00        34\n",
      "           aid_centers       0.00      0.00      0.00       115\n",
      "  other_infrastructure       0.67      0.01      0.01       383\n",
      "       weather_related       0.83      0.52      0.64      2390\n",
      "                floods       0.86      0.28      0.43       693\n",
      "                 storm       0.73      0.28      0.41       812\n",
      "                  fire       0.82      0.10      0.18        90\n",
      "            earthquake       0.86      0.49      0.62       787\n",
      "                  cold       0.53      0.05      0.10       187\n",
      "         other_weather       0.43      0.02      0.04       452\n",
      "         direct_report       0.76      0.28      0.41      1694\n",
      "\n",
      "             micro avg       0.79      0.43      0.56     27308\n",
      "             macro avg       0.52      0.15      0.20     27308\n",
      "          weighted avg       0.71      0.43      0.49     27308\n",
      "           samples avg       0.65      0.41      0.46     27308\n",
      "\n",
      "______________________________Model______________________________\n",
      "______________________________ GradientBoostingClassifier ______________________________\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related       0.80      0.97      0.88      6534\n",
      "               request       0.84      0.52      0.65      1472\n",
      "                 offer       0.00      0.00      0.00        38\n",
      "           aid_related       0.77      0.57      0.65      3545\n",
      "          medical_help       0.66      0.22      0.33       701\n",
      "      medical_products       0.62      0.28      0.39       446\n",
      "     search_and_rescue       0.31      0.18      0.22       226\n",
      "              security       0.05      0.03      0.03       160\n",
      "              military       0.50      0.21      0.29       267\n",
      "                 water       0.71      0.64      0.67       543\n",
      "                  food       0.80      0.79      0.79       965\n",
      "               shelter       0.79      0.54      0.64       775\n",
      "              clothing       0.54      0.50      0.52       127\n",
      "                 money       0.42      0.24      0.31       191\n",
      "        missing_people       0.38      0.26      0.31       104\n",
      "              refugees       0.44      0.22      0.29       293\n",
      "                 death       0.72      0.49      0.58       406\n",
      "             other_aid       0.57      0.09      0.15      1139\n",
      "infrastructure_related       0.42      0.03      0.05       568\n",
      "             transport       0.54      0.23      0.33       407\n",
      "             buildings       0.73      0.29      0.42       441\n",
      "           electricity       0.40      0.22      0.29       185\n",
      "                 tools       0.07      0.04      0.05        53\n",
      "             hospitals       0.09      0.07      0.08        85\n",
      "                 shops       0.00      0.00      0.00        34\n",
      "           aid_centers       0.16      0.10      0.12       115\n",
      "  other_infrastructure       0.20      0.02      0.04       383\n",
      "       weather_related       0.87      0.59      0.71      2390\n",
      "                floods       0.86      0.53      0.66       693\n",
      "                 storm       0.76      0.59      0.66       812\n",
      "                  fire       0.36      0.29      0.32        90\n",
      "            earthquake       0.83      0.79      0.81       787\n",
      "                  cold       0.53      0.35      0.42       187\n",
      "         other_weather       0.47      0.11      0.18       452\n",
      "         direct_report       0.79      0.43      0.56      1694\n",
      "\n",
      "             micro avg       0.76      0.57      0.65     27308\n",
      "             macro avg       0.51      0.33      0.38     27308\n",
      "          weighted avg       0.72      0.57      0.61     27308\n",
      "           samples avg       0.65      0.50      0.52     27308\n",
      "\n",
      "______________________________Model______________________________\n",
      "______________________________ AdaBoostClassifier ______________________________\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related       0.81      0.94      0.87      6534\n",
      "               request       0.78      0.54      0.64      1472\n",
      "                 offer       0.09      0.03      0.04        38\n",
      "           aid_related       0.75      0.60      0.66      3545\n",
      "          medical_help       0.57      0.26      0.36       701\n",
      "      medical_products       0.63      0.30      0.41       446\n",
      "     search_and_rescue       0.55      0.17      0.26       226\n",
      "              security       0.20      0.03      0.04       160\n",
      "              military       0.53      0.28      0.37       267\n",
      "                 water       0.72      0.61      0.66       543\n",
      "                  food       0.80      0.68      0.74       965\n",
      "               shelter       0.76      0.57      0.65       775\n",
      "              clothing       0.65      0.40      0.50       127\n",
      "                 money       0.49      0.28      0.35       191\n",
      "        missing_people       0.65      0.11      0.18       104\n",
      "              refugees       0.60      0.29      0.39       293\n",
      "                 death       0.78      0.42      0.54       406\n",
      "             other_aid       0.55      0.16      0.25      1139\n",
      "infrastructure_related       0.46      0.09      0.15       568\n",
      "             transport       0.64      0.16      0.26       407\n",
      "             buildings       0.69      0.37      0.48       441\n",
      "           electricity       0.51      0.19      0.28       185\n",
      "                 tools       0.15      0.04      0.06        53\n",
      "             hospitals       0.27      0.08      0.13        85\n",
      "                 shops       0.09      0.03      0.04        34\n",
      "           aid_centers       0.27      0.06      0.10       115\n",
      "  other_infrastructure       0.42      0.09      0.15       383\n",
      "       weather_related       0.85      0.65      0.74      2390\n",
      "                floods       0.86      0.56      0.68       693\n",
      "                 storm       0.73      0.50      0.60       812\n",
      "                  fire       0.53      0.20      0.29        90\n",
      "            earthquake       0.87      0.75      0.80       787\n",
      "                  cold       0.74      0.34      0.47       187\n",
      "         other_weather       0.41      0.12      0.19       452\n",
      "         direct_report       0.71      0.47      0.57      1694\n",
      "\n",
      "             micro avg       0.76      0.58      0.66     27308\n",
      "             macro avg       0.57      0.33      0.40     27308\n",
      "          weighted avg       0.73      0.58      0.62     27308\n",
      "           samples avg       0.63      0.50      0.51     27308\n",
      "\n",
      "______________________________Model______________________________\n",
      "______________________________ SVC ______________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related       0.76      1.00      0.86      6534\n",
      "               request       0.00      0.00      0.00      1472\n",
      "                 offer       0.00      0.00      0.00        38\n",
      "           aid_related       0.00      0.00      0.00      3545\n",
      "          medical_help       0.00      0.00      0.00       701\n",
      "      medical_products       0.00      0.00      0.00       446\n",
      "     search_and_rescue       0.00      0.00      0.00       226\n",
      "              security       0.00      0.00      0.00       160\n",
      "              military       0.00      0.00      0.00       267\n",
      "                 water       0.00      0.00      0.00       543\n",
      "                  food       0.00      0.00      0.00       965\n",
      "               shelter       0.00      0.00      0.00       775\n",
      "              clothing       0.00      0.00      0.00       127\n",
      "                 money       0.00      0.00      0.00       191\n",
      "        missing_people       0.00      0.00      0.00       104\n",
      "              refugees       0.00      0.00      0.00       293\n",
      "                 death       0.00      0.00      0.00       406\n",
      "             other_aid       0.00      0.00      0.00      1139\n",
      "infrastructure_related       0.00      0.00      0.00       568\n",
      "             transport       0.00      0.00      0.00       407\n",
      "             buildings       0.00      0.00      0.00       441\n",
      "           electricity       0.00      0.00      0.00       185\n",
      "                 tools       0.00      0.00      0.00        53\n",
      "             hospitals       0.00      0.00      0.00        85\n",
      "                 shops       0.00      0.00      0.00        34\n",
      "           aid_centers       0.00      0.00      0.00       115\n",
      "  other_infrastructure       0.00      0.00      0.00       383\n",
      "       weather_related       0.00      0.00      0.00      2390\n",
      "                floods       0.00      0.00      0.00       693\n",
      "                 storm       0.00      0.00      0.00       812\n",
      "                  fire       0.00      0.00      0.00        90\n",
      "            earthquake       0.00      0.00      0.00       787\n",
      "                  cold       0.00      0.00      0.00       187\n",
      "         other_weather       0.00      0.00      0.00       452\n",
      "         direct_report       0.00      0.00      0.00      1694\n",
      "\n",
      "             micro avg       0.76      0.24      0.36     27308\n",
      "             macro avg       0.02      0.03      0.02     27308\n",
      "          weighted avg       0.18      0.24      0.21     27308\n",
      "           samples avg       0.76      0.32      0.40     27308\n",
      "\n"
     ]
    }
   ],
   "source": [
    "perf_report(fitted_mdls, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-shops has very little label diversity so it became an edge case, I will drop it for the optimization\n",
    "\n",
    "______________________________ RandomForestClassifier ______________________________\n",
    "\n",
    "\n",
    "                           precision    recall  f1-score   support\n",
    "                           \n",
    "                           \n",
    "             micro avg       0.80      0.44      0.57     27308\n",
    "             \n",
    "             \n",
    "             macro avg       0.58      0.16      0.21     27308\n",
    "             \n",
    "             \n",
    "          weighted avg       0.74      0.44      0.50     27308\n",
    "          \n",
    "          \n",
    "           samples avg       0.65      0.42      0.46     27308\n",
    "           \n",
    "\n",
    "______________________________ ExtraTreesClassifier ______________________________\n",
    "\n",
    "\n",
    "             micro avg       0.79      0.44      0.56     27308\n",
    "             \n",
    "             \n",
    "             macro avg       0.53      0.15      0.21     27308\n",
    "             \n",
    "             \n",
    "          weighted avg       0.71      0.44      0.49     27308\n",
    "          \n",
    "          \n",
    "           samples avg       0.66      0.42      0.46     27308\n",
    "\n",
    "______________________________ GradientBoostingClassifier ______________________________\n",
    "\n",
    "\n",
    "             micro avg       0.76      0.57      0.65     27308\n",
    "             \n",
    "             \n",
    "             macro avg       0.51      0.32      0.38     27308\n",
    "             \n",
    "             \n",
    "          weighted avg       0.72      0.57      0.61     27308\n",
    "          \n",
    "          \n",
    "           samples avg       0.65      0.50      0.52     27308\n",
    "           \n",
    "           \n",
    "______________________________ AdaBoostClassifier ______________________________\n",
    "\n",
    "\n",
    "             micro avg       0.77      0.58      0.66     27308\n",
    "             \n",
    "             \n",
    "             macro avg       0.58      0.33      0.40     27308\n",
    "             \n",
    "             \n",
    "          weighted avg       0.73      0.58      0.62     27308\n",
    "          \n",
    "          \n",
    "           samples avg       0.63      0.50      0.51     27308\n",
    "           \n",
    "______________________________ SVC ______________________________\n",
    "\n",
    "\n",
    "             micro avg       0.76      0.24      0.36     27308\n",
    "             \n",
    "             \n",
    "             macro avg       0.02      0.03      0.02     27308\n",
    "             \n",
    "             \n",
    "          weighted avg       0.18      0.24      0.21     27308\n",
    "          \n",
    "          \n",
    "           samples avg       0.76      0.32      0.40     27308\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve models based on poor target performance elimination\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each.\n",
    "Testing models after dropping poor predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the targets that had the word performances based on the classification report\n",
    "targs_drop = ['offer', 'security', 'infrastructure_related', 'tools', \n",
    "              'hospitals', 'shops', 'aid_centers', 'other_infrastructure', 'fire', 'other_weather']\n",
    "y_min = y.copy()\n",
    "y_min.drop(targs_drop, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_min, random_state = 42, test_size = 0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  RandomForestClassifier\n",
      "{'memory': None, 'steps': [('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=<function tokenize at 0x0000028973392D38>,\n",
      "                vocabulary=None)), ('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)), ('clf', MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True,\n",
      "                                                       class_weight=None,\n",
      "                                                       criterion='gini',\n",
      "                                                       max_depth=None,\n",
      "                                                       max_features='auto',\n",
      "                                                       max_leaf_nodes=None,\n",
      "                                                       min_impurity_decrease=0.0,\n",
      "                                                       min_impurity_split=None,\n",
      "                                                       min_samples_leaf=1,\n",
      "                                                       min_samples_split=2,\n",
      "                                                       min_weight_fraction_leaf=0.0,\n",
      "                                                       n_estimators='warn',\n",
      "                                                       n_jobs=None,\n",
      "                                                       oob_score=False,\n",
      "                                                       random_state=None,\n",
      "                                                       verbose=0,\n",
      "                                                       warm_start=False),\n",
      "                      n_jobs=None))], 'verbose': False, 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=<function tokenize at 0x0000028973392D38>,\n",
      "                vocabulary=None), 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True), 'clf': MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True,\n",
      "                                                       class_weight=None,\n",
      "                                                       criterion='gini',\n",
      "                                                       max_depth=None,\n",
      "                                                       max_features='auto',\n",
      "                                                       max_leaf_nodes=None,\n",
      "                                                       min_impurity_decrease=0.0,\n",
      "                                                       min_impurity_split=None,\n",
      "                                                       min_samples_leaf=1,\n",
      "                                                       min_samples_split=2,\n",
      "                                                       min_weight_fraction_leaf=0.0,\n",
      "                                                       n_estimators='warn',\n",
      "                                                       n_jobs=None,\n",
      "                                                       oob_score=False,\n",
      "                                                       random_state=None,\n",
      "                                                       verbose=0,\n",
      "                                                       warm_start=False),\n",
      "                      n_jobs=None), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': <function tokenize at 0x0000028973392D38>, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'clf__estimator__bootstrap': True, 'clf__estimator__class_weight': None, 'clf__estimator__criterion': 'gini', 'clf__estimator__max_depth': None, 'clf__estimator__max_features': 'auto', 'clf__estimator__max_leaf_nodes': None, 'clf__estimator__min_impurity_decrease': 0.0, 'clf__estimator__min_impurity_split': None, 'clf__estimator__min_samples_leaf': 1, 'clf__estimator__min_samples_split': 2, 'clf__estimator__min_weight_fraction_leaf': 0.0, 'clf__estimator__n_estimators': 'warn', 'clf__estimator__n_jobs': None, 'clf__estimator__oob_score': False, 'clf__estimator__random_state': None, 'clf__estimator__verbose': 0, 'clf__estimator__warm_start': False, 'clf__estimator': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators='warn',\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False), 'clf__n_jobs': None}\n",
      "Model:  ExtraTreesClassifier\n",
      "{'memory': None, 'steps': [('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=<function tokenize at 0x0000028973392D38>,\n",
      "                vocabulary=None)), ('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)), ('clf', MultiOutputClassifier(estimator=ExtraTreesClassifier(bootstrap=False,\n",
      "                                                     class_weight=None,\n",
      "                                                     criterion='gini',\n",
      "                                                     max_depth=None,\n",
      "                                                     max_features='auto',\n",
      "                                                     max_leaf_nodes=None,\n",
      "                                                     min_impurity_decrease=0.0,\n",
      "                                                     min_impurity_split=None,\n",
      "                                                     min_samples_leaf=1,\n",
      "                                                     min_samples_split=2,\n",
      "                                                     min_weight_fraction_leaf=0.0,\n",
      "                                                     n_estimators='warn',\n",
      "                                                     n_jobs=None,\n",
      "                                                     oob_score=False,\n",
      "                                                     random_state=None,\n",
      "                                                     verbose=0,\n",
      "                                                     warm_start=False),\n",
      "                      n_jobs=None))], 'verbose': False, 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=<function tokenize at 0x0000028973392D38>,\n",
      "                vocabulary=None), 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True), 'clf': MultiOutputClassifier(estimator=ExtraTreesClassifier(bootstrap=False,\n",
      "                                                     class_weight=None,\n",
      "                                                     criterion='gini',\n",
      "                                                     max_depth=None,\n",
      "                                                     max_features='auto',\n",
      "                                                     max_leaf_nodes=None,\n",
      "                                                     min_impurity_decrease=0.0,\n",
      "                                                     min_impurity_split=None,\n",
      "                                                     min_samples_leaf=1,\n",
      "                                                     min_samples_split=2,\n",
      "                                                     min_weight_fraction_leaf=0.0,\n",
      "                                                     n_estimators='warn',\n",
      "                                                     n_jobs=None,\n",
      "                                                     oob_score=False,\n",
      "                                                     random_state=None,\n",
      "                                                     verbose=0,\n",
      "                                                     warm_start=False),\n",
      "                      n_jobs=None), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': <function tokenize at 0x0000028973392D38>, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'clf__estimator__bootstrap': False, 'clf__estimator__class_weight': None, 'clf__estimator__criterion': 'gini', 'clf__estimator__max_depth': None, 'clf__estimator__max_features': 'auto', 'clf__estimator__max_leaf_nodes': None, 'clf__estimator__min_impurity_decrease': 0.0, 'clf__estimator__min_impurity_split': None, 'clf__estimator__min_samples_leaf': 1, 'clf__estimator__min_samples_split': 2, 'clf__estimator__min_weight_fraction_leaf': 0.0, 'clf__estimator__n_estimators': 'warn', 'clf__estimator__n_jobs': None, 'clf__estimator__oob_score': False, 'clf__estimator__random_state': None, 'clf__estimator__verbose': 0, 'clf__estimator__warm_start': False, 'clf__estimator': ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "                     max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                     min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                     min_samples_leaf=1, min_samples_split=2,\n",
      "                     min_weight_fraction_leaf=0.0, n_estimators='warn',\n",
      "                     n_jobs=None, oob_score=False, random_state=None, verbose=0,\n",
      "                     warm_start=False), 'clf__n_jobs': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  GradientBoostingClassifier\n",
      "{'memory': None, 'steps': [('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=<function tokenize at 0x0000028973392D38>,\n",
      "                vocabulary=None)), ('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)), ('clf', MultiOutputClassifier(estimator=GradientBoostingClassifier(criterion='friedman_mse',\n",
      "                                                           init=None,\n",
      "                                                           learning_rate=0.1,\n",
      "                                                           loss='deviance',\n",
      "                                                           max_depth=3,\n",
      "                                                           max_features=None,\n",
      "                                                           max_leaf_nodes=None,\n",
      "                                                           min_impurity_decrease=0.0,\n",
      "                                                           min_impurity_split=None,\n",
      "                                                           min_samples_leaf=1,\n",
      "                                                           min_samples_split=2,\n",
      "                                                           min_weight_fraction_leaf=0.0,\n",
      "                                                           n_estimators=100,\n",
      "                                                           n_iter_no_change=None,\n",
      "                                                           presort='auto',\n",
      "                                                           random_state=None,\n",
      "                                                           subsample=1.0,\n",
      "                                                           tol=0.0001,\n",
      "                                                           validation_fraction=0.1,\n",
      "                                                           verbose=0,\n",
      "                                                           warm_start=False),\n",
      "                      n_jobs=None))], 'verbose': False, 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=<function tokenize at 0x0000028973392D38>,\n",
      "                vocabulary=None), 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True), 'clf': MultiOutputClassifier(estimator=GradientBoostingClassifier(criterion='friedman_mse',\n",
      "                                                           init=None,\n",
      "                                                           learning_rate=0.1,\n",
      "                                                           loss='deviance',\n",
      "                                                           max_depth=3,\n",
      "                                                           max_features=None,\n",
      "                                                           max_leaf_nodes=None,\n",
      "                                                           min_impurity_decrease=0.0,\n",
      "                                                           min_impurity_split=None,\n",
      "                                                           min_samples_leaf=1,\n",
      "                                                           min_samples_split=2,\n",
      "                                                           min_weight_fraction_leaf=0.0,\n",
      "                                                           n_estimators=100,\n",
      "                                                           n_iter_no_change=None,\n",
      "                                                           presort='auto',\n",
      "                                                           random_state=None,\n",
      "                                                           subsample=1.0,\n",
      "                                                           tol=0.0001,\n",
      "                                                           validation_fraction=0.1,\n",
      "                                                           verbose=0,\n",
      "                                                           warm_start=False),\n",
      "                      n_jobs=None), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': <function tokenize at 0x0000028973392D38>, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'clf__estimator__criterion': 'friedman_mse', 'clf__estimator__init': None, 'clf__estimator__learning_rate': 0.1, 'clf__estimator__loss': 'deviance', 'clf__estimator__max_depth': 3, 'clf__estimator__max_features': None, 'clf__estimator__max_leaf_nodes': None, 'clf__estimator__min_impurity_decrease': 0.0, 'clf__estimator__min_impurity_split': None, 'clf__estimator__min_samples_leaf': 1, 'clf__estimator__min_samples_split': 2, 'clf__estimator__min_weight_fraction_leaf': 0.0, 'clf__estimator__n_estimators': 100, 'clf__estimator__n_iter_no_change': None, 'clf__estimator__presort': 'auto', 'clf__estimator__random_state': None, 'clf__estimator__subsample': 1.0, 'clf__estimator__tol': 0.0001, 'clf__estimator__validation_fraction': 0.1, 'clf__estimator__verbose': 0, 'clf__estimator__warm_start': False, 'clf__estimator': GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "                           max_features=None, max_leaf_nodes=None,\n",
      "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                           min_samples_leaf=1, min_samples_split=2,\n",
      "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                           n_iter_no_change=None, presort='auto',\n",
      "                           random_state=None, subsample=1.0, tol=0.0001,\n",
      "                           validation_fraction=0.1, verbose=0,\n",
      "                           warm_start=False), 'clf__n_jobs': None}\n",
      "Model:  AdaBoostClassifier\n",
      "{'memory': None, 'steps': [('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=<function tokenize at 0x0000028973392D38>,\n",
      "                vocabulary=None)), ('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)), ('clf', MultiOutputClassifier(estimator=AdaBoostClassifier(algorithm='SAMME.R',\n",
      "                                                   base_estimator=None,\n",
      "                                                   learning_rate=1.0,\n",
      "                                                   n_estimators=50,\n",
      "                                                   random_state=None),\n",
      "                      n_jobs=None))], 'verbose': False, 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=<function tokenize at 0x0000028973392D38>,\n",
      "                vocabulary=None), 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True), 'clf': MultiOutputClassifier(estimator=AdaBoostClassifier(algorithm='SAMME.R',\n",
      "                                                   base_estimator=None,\n",
      "                                                   learning_rate=1.0,\n",
      "                                                   n_estimators=50,\n",
      "                                                   random_state=None),\n",
      "                      n_jobs=None), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': <function tokenize at 0x0000028973392D38>, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'clf__estimator__algorithm': 'SAMME.R', 'clf__estimator__base_estimator': None, 'clf__estimator__learning_rate': 1.0, 'clf__estimator__n_estimators': 50, 'clf__estimator__random_state': None, 'clf__estimator': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
      "                   n_estimators=50, random_state=None), 'clf__n_jobs': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  SVC\n",
      "{'memory': None, 'steps': [('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=<function tokenize at 0x0000028973392D38>,\n",
      "                vocabulary=None)), ('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)), ('clf', MultiOutputClassifier(estimator=SVC(C=1.0, cache_size=200, class_weight=None,\n",
      "                                    coef0=0.0, decision_function_shape='ovr',\n",
      "                                    degree=3, gamma='auto_deprecated',\n",
      "                                    kernel='rbf', max_iter=-1,\n",
      "                                    probability=False, random_state=None,\n",
      "                                    shrinking=True, tol=0.001, verbose=False),\n",
      "                      n_jobs=None))], 'verbose': False, 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=<function tokenize at 0x0000028973392D38>,\n",
      "                vocabulary=None), 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True), 'clf': MultiOutputClassifier(estimator=SVC(C=1.0, cache_size=200, class_weight=None,\n",
      "                                    coef0=0.0, decision_function_shape='ovr',\n",
      "                                    degree=3, gamma='auto_deprecated',\n",
      "                                    kernel='rbf', max_iter=-1,\n",
      "                                    probability=False, random_state=None,\n",
      "                                    shrinking=True, tol=0.001, verbose=False),\n",
      "                      n_jobs=None), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': None, 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': <function tokenize at 0x0000028973392D38>, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'clf__estimator__C': 1.0, 'clf__estimator__cache_size': 200, 'clf__estimator__class_weight': None, 'clf__estimator__coef0': 0.0, 'clf__estimator__decision_function_shape': 'ovr', 'clf__estimator__degree': 3, 'clf__estimator__gamma': 'auto_deprecated', 'clf__estimator__kernel': 'rbf', 'clf__estimator__max_iter': -1, 'clf__estimator__probability': False, 'clf__estimator__random_state': None, 'clf__estimator__shrinking': True, 'clf__estimator__tol': 0.001, 'clf__estimator__verbose': False, 'clf__estimator': SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "    kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
      "    shrinking=True, tol=0.001, verbose=False), 'clf__n_jobs': None}\n"
     ]
    }
   ],
   "source": [
    "fitted_mdls_min = multi_tester(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = y_train.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________Model______________________________\n",
      "______________________________ RandomForestClassifier ______________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "          related       0.81      0.93      0.86      6534\n",
      "          request       0.84      0.38      0.52      1472\n",
      "      aid_related       0.74      0.51      0.61      3545\n",
      "     medical_help       0.61      0.06      0.12       701\n",
      " medical_products       0.69      0.08      0.15       446\n",
      "search_and_rescue       0.20      0.01      0.02       226\n",
      "         military       0.51      0.07      0.12       267\n",
      "            water       0.80      0.17      0.28       543\n",
      "             food       0.89      0.32      0.47       965\n",
      "          shelter       0.82      0.22      0.35       775\n",
      "         clothing       0.80      0.06      0.12       127\n",
      "            money       0.62      0.03      0.05       191\n",
      "   missing_people       1.00      0.03      0.06       104\n",
      "         refugees       0.33      0.01      0.01       293\n",
      "            death       0.86      0.15      0.26       406\n",
      "        other_aid       0.58      0.04      0.08      1139\n",
      "        transport       0.76      0.07      0.13       407\n",
      "        buildings       0.79      0.07      0.13       441\n",
      "      electricity       0.67      0.01      0.02       185\n",
      "  weather_related       0.84      0.50      0.63      2390\n",
      "           floods       0.89      0.34      0.49       693\n",
      "            storm       0.75      0.37      0.49       812\n",
      "       earthquake       0.89      0.63      0.74       787\n",
      "             cold       0.84      0.09      0.16       187\n",
      "    direct_report       0.78      0.30      0.44      1694\n",
      "\n",
      "        micro avg       0.80      0.48      0.60     25330\n",
      "        macro avg       0.73      0.22      0.29     25330\n",
      "     weighted avg       0.77      0.48      0.54     25330\n",
      "      samples avg       0.66      0.43      0.48     25330\n",
      "\n",
      "______________________________Model______________________________\n",
      "______________________________ ExtraTreesClassifier ______________________________\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "          related       0.82      0.93      0.87      6534\n",
      "          request       0.84      0.36      0.50      1472\n",
      "      aid_related       0.74      0.51      0.60      3545\n",
      "     medical_help       0.51      0.07      0.12       701\n",
      " medical_products       0.65      0.08      0.14       446\n",
      "search_and_rescue       0.35      0.04      0.06       226\n",
      "         military       0.57      0.13      0.21       267\n",
      "            water       0.79      0.20      0.32       543\n",
      "             food       0.83      0.26      0.39       965\n",
      "          shelter       0.76      0.22      0.34       775\n",
      "         clothing       0.64      0.06      0.10       127\n",
      "            money       0.71      0.05      0.10       191\n",
      "   missing_people       0.50      0.01      0.02       104\n",
      "         refugees       0.35      0.02      0.04       293\n",
      "            death       0.73      0.14      0.23       406\n",
      "        other_aid       0.39      0.03      0.05      1139\n",
      "        transport       0.41      0.05      0.08       407\n",
      "        buildings       0.63      0.08      0.14       441\n",
      "      electricity       0.38      0.02      0.03       185\n",
      "  weather_related       0.83      0.49      0.61      2390\n",
      "           floods       0.85      0.26      0.40       693\n",
      "            storm       0.75      0.29      0.42       812\n",
      "       earthquake       0.87      0.52      0.65       787\n",
      "             cold       0.54      0.07      0.13       187\n",
      "    direct_report       0.77      0.28      0.41      1694\n",
      "\n",
      "        micro avg       0.79      0.46      0.59     25330\n",
      "        macro avg       0.65      0.21      0.28     25330\n",
      "     weighted avg       0.74      0.46      0.52     25330\n",
      "      samples avg       0.66      0.43      0.47     25330\n",
      "\n",
      "______________________________Model______________________________\n",
      "______________________________ GradientBoostingClassifier ______________________________\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "          related       0.80      0.97      0.88      6534\n",
      "          request       0.84      0.52      0.64      1472\n",
      "      aid_related       0.77      0.56      0.65      3545\n",
      "     medical_help       0.63      0.21      0.32       701\n",
      " medical_products       0.60      0.28      0.38       446\n",
      "search_and_rescue       0.30      0.17      0.22       226\n",
      "         military       0.50      0.21      0.30       267\n",
      "            water       0.74      0.65      0.69       543\n",
      "             food       0.79      0.79      0.79       965\n",
      "          shelter       0.79      0.53      0.63       775\n",
      "         clothing       0.51      0.46      0.49       127\n",
      "            money       0.45      0.24      0.31       191\n",
      "   missing_people       0.34      0.26      0.29       104\n",
      "         refugees       0.44      0.23      0.30       293\n",
      "            death       0.70      0.49      0.57       406\n",
      "        other_aid       0.60      0.09      0.15      1139\n",
      "        transport       0.52      0.23      0.32       407\n",
      "        buildings       0.70      0.29      0.41       441\n",
      "      electricity       0.37      0.22      0.27       185\n",
      "  weather_related       0.87      0.59      0.70      2390\n",
      "           floods       0.87      0.53      0.66       693\n",
      "            storm       0.76      0.60      0.67       812\n",
      "       earthquake       0.84      0.79      0.81       787\n",
      "             cold       0.52      0.35      0.42       187\n",
      "    direct_report       0.79      0.43      0.56      1694\n",
      "\n",
      "        micro avg       0.77      0.61      0.68     25330\n",
      "        macro avg       0.64      0.43      0.50     25330\n",
      "     weighted avg       0.76      0.61      0.65     25330\n",
      "      samples avg       0.66      0.52      0.53     25330\n",
      "\n",
      "______________________________Model______________________________\n",
      "______________________________ AdaBoostClassifier ______________________________\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "          related       0.81      0.94      0.87      6534\n",
      "          request       0.78      0.54      0.64      1472\n",
      "      aid_related       0.75      0.60      0.66      3545\n",
      "     medical_help       0.57      0.26      0.36       701\n",
      " medical_products       0.63      0.30      0.41       446\n",
      "search_and_rescue       0.55      0.17      0.26       226\n",
      "         military       0.53      0.28      0.37       267\n",
      "            water       0.72      0.61      0.66       543\n",
      "             food       0.80      0.68      0.74       965\n",
      "          shelter       0.76      0.57      0.65       775\n",
      "         clothing       0.65      0.40      0.50       127\n",
      "            money       0.49      0.28      0.35       191\n",
      "   missing_people       0.65      0.11      0.18       104\n",
      "         refugees       0.60      0.29      0.39       293\n",
      "            death       0.78      0.42      0.54       406\n",
      "        other_aid       0.55      0.16      0.25      1139\n",
      "        transport       0.64      0.16      0.26       407\n",
      "        buildings       0.69      0.37      0.48       441\n",
      "      electricity       0.51      0.19      0.28       185\n",
      "  weather_related       0.85      0.65      0.74      2390\n",
      "           floods       0.86      0.56      0.68       693\n",
      "            storm       0.73      0.50      0.60       812\n",
      "       earthquake       0.87      0.75      0.80       787\n",
      "             cold       0.74      0.34      0.47       187\n",
      "    direct_report       0.71      0.47      0.57      1694\n",
      "\n",
      "        micro avg       0.77      0.61      0.69     25330\n",
      "        macro avg       0.69      0.42      0.51     25330\n",
      "     weighted avg       0.75      0.61      0.66     25330\n",
      "      samples avg       0.64      0.51      0.53     25330\n",
      "\n",
      "______________________________Model______________________________\n",
      "______________________________ SVC ______________________________\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "          related       0.76      1.00      0.86      6534\n",
      "          request       0.00      0.00      0.00      1472\n",
      "      aid_related       0.00      0.00      0.00      3545\n",
      "     medical_help       0.00      0.00      0.00       701\n",
      " medical_products       0.00      0.00      0.00       446\n",
      "search_and_rescue       0.00      0.00      0.00       226\n",
      "         military       0.00      0.00      0.00       267\n",
      "            water       0.00      0.00      0.00       543\n",
      "             food       0.00      0.00      0.00       965\n",
      "          shelter       0.00      0.00      0.00       775\n",
      "         clothing       0.00      0.00      0.00       127\n",
      "            money       0.00      0.00      0.00       191\n",
      "   missing_people       0.00      0.00      0.00       104\n",
      "         refugees       0.00      0.00      0.00       293\n",
      "            death       0.00      0.00      0.00       406\n",
      "        other_aid       0.00      0.00      0.00      1139\n",
      "        transport       0.00      0.00      0.00       407\n",
      "        buildings       0.00      0.00      0.00       441\n",
      "      electricity       0.00      0.00      0.00       185\n",
      "  weather_related       0.00      0.00      0.00      2390\n",
      "           floods       0.00      0.00      0.00       693\n",
      "            storm       0.00      0.00      0.00       812\n",
      "       earthquake       0.00      0.00      0.00       787\n",
      "             cold       0.00      0.00      0.00       187\n",
      "    direct_report       0.00      0.00      0.00      1694\n",
      "\n",
      "        micro avg       0.76      0.26      0.38     25330\n",
      "        macro avg       0.03      0.04      0.03     25330\n",
      "     weighted avg       0.19      0.26      0.22     25330\n",
      "      samples avg       0.76      0.33      0.41     25330\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "perf_report(fitted_mdls_min, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "______________________________ RandomForestClassifier ______________________________\n",
    "\n",
    "                           precision    recall  f1-score   support \n",
    "                           \n",
    "              micro avg       0.80      0.48      0.60     25330\n",
    "              \n",
    "              macro avg       0.72      0.22      0.30     25330\n",
    "              \n",
    "           weighted avg       0.78      0.48      0.53     25330\n",
    "           \n",
    "            samples avg       0.66      0.44      0.48     25330\n",
    "\n",
    "______________________________ ExtraTreesClassifier ______________________________\n",
    "\n",
    "        micro avg       0.79      0.46      0.59     25330\n",
    "        \n",
    "        macro avg       0.68      0.20      0.27     25330\n",
    "        \n",
    "     weighted avg       0.75      0.46      0.52     25330\n",
    "     \n",
    "      samples avg       0.65      0.43      0.47     25330    \n",
    "\n",
    "______________________________ GradientBoostingClassifier ______________________________\n",
    "\n",
    "        micro avg       0.78      0.61      0.68     25330\n",
    "        \n",
    "        macro avg       0.65      0.43      0.50     25330\n",
    "        \n",
    "     weighted avg       0.76      0.61      0.65     25330\n",
    "     \n",
    "      samples avg       0.66      0.52      0.54     25330\n",
    "           \n",
    "           \n",
    "______________________________ AdaBoostClassifier ______________________________\n",
    "\n",
    "        micro avg       0.77      0.61      0.69     25330\n",
    "        \n",
    "        macro avg       0.69      0.42      0.51     25330\n",
    "        \n",
    "     weighted avg       0.75      0.61      0.66     25330\n",
    "     \n",
    "      samples avg       0.64      0.51      0.53     25330\n",
    "           \n",
    "______________________________ SVC ______________________________\n",
    "\n",
    "        micro avg       0.76      0.26      0.38     25330\n",
    "        \n",
    "        macro avg       0.03      0.04      0.03     25330\n",
    "        \n",
    "     weighted avg       0.19      0.26      0.22     25330\n",
    "     \n",
    "      samples avg       0.76      0.33      0.41     25330\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Improve your model\n",
    "Use grid search to find better parameters. \n",
    "\n",
    "I will work on my best performing model adaboost and using the reduced target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(AdaBoostClassifier()))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('vect',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function tokenize at 0x0000028973392D38>,\n",
       "                   vocabulary=None)),\n",
       "  ('tfidf',\n",
       "   TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),\n",
       "  ('clf',\n",
       "   MultiOutputClassifier(estimator=AdaBoostClassifier(algorithm='SAMME.R',\n",
       "                                                      base_estimator=None,\n",
       "                                                      learning_rate=1.0,\n",
       "                                                      n_estimators=50,\n",
       "                                                      random_state=None),\n",
       "                         n_jobs=None))],\n",
       " 'verbose': False,\n",
       " 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function tokenize at 0x0000028973392D38>,\n",
       "                 vocabulary=None),\n",
       " 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
       " 'clf': MultiOutputClassifier(estimator=AdaBoostClassifier(algorithm='SAMME.R',\n",
       "                                                    base_estimator=None,\n",
       "                                                    learning_rate=1.0,\n",
       "                                                    n_estimators=50,\n",
       "                                                    random_state=None),\n",
       "                       n_jobs=None),\n",
       " 'vect__analyzer': 'word',\n",
       " 'vect__binary': False,\n",
       " 'vect__decode_error': 'strict',\n",
       " 'vect__dtype': numpy.int64,\n",
       " 'vect__encoding': 'utf-8',\n",
       " 'vect__input': 'content',\n",
       " 'vect__lowercase': True,\n",
       " 'vect__max_df': 1.0,\n",
       " 'vect__max_features': None,\n",
       " 'vect__min_df': 1,\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__preprocessor': None,\n",
       " 'vect__stop_words': None,\n",
       " 'vect__strip_accents': None,\n",
       " 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vect__tokenizer': <function __main__.tokenize(text)>,\n",
       " 'vect__vocabulary': None,\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__smooth_idf': True,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__use_idf': True,\n",
       " 'clf__estimator__algorithm': 'SAMME.R',\n",
       " 'clf__estimator__base_estimator': None,\n",
       " 'clf__estimator__learning_rate': 1.0,\n",
       " 'clf__estimator__n_estimators': 50,\n",
       " 'clf__estimator__random_state': None,\n",
       " 'clf__estimator': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None),\n",
       " 'clf__n_jobs': None}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'tfidf__use_idf': (True, False),\n",
    "              'clf__estimator__n_estimators': [50, 100], \n",
    "              'clf__estimator__random_state': [42],\n",
    "             'clf__estimator__learning_rate': [0.5]} \n",
    "\n",
    "cv = GridSearchCV(pipeline, param_grid = parameters, cv = 10,\n",
    "                  refit = True, verbose = 1, return_train_score = True, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('vect',\n",
       "                                        CountVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.int64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        preprocessor=None,\n",
       "                                                        stop_words=None,\n",
       "                                                        strip_accents=None,\n",
       "                                                        to...\n",
       "                                                                                           base_estimator=None,\n",
       "                                                                                           learning_rate=1.0,\n",
       "                                                                                           n_estimators=50,\n",
       "                                                                                           random_state=None),\n",
       "                                                              n_jobs=None))],\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'clf__estimator__learning_rate': [0.5],\n",
       "                         'clf__estimator__n_estimators': [50, 100],\n",
       "                         'clf__estimator__random_state': [42],\n",
       "                         'tfidf__use_idf': (True, False)},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "             scoring=None, verbose=1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Test selected model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the targets that had the word performances based on the classification report\n",
    "targs_drop = ['offer', 'security', 'infrastructure_related', 'tools', \n",
    "              'hospitals', 'shops', 'aid_centers', 'other_infrastructure', 'fire', 'other_weather']\n",
    "y_min = y.copy()\n",
    "y_min.drop(targs_drop, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_min, random_state = 42, test_size = 0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed: 31.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model : 0.2847870644500114\n",
      "Params : {'clf__estimator__learning_rate': 0.5, 'clf__estimator__n_estimators': 100, 'clf__estimator__random_state': 42, 'tfidf__use_idf': False}\n"
     ]
    }
   ],
   "source": [
    "best_ada = cv.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print('Best model :', best_ada.best_score_)\n",
    "print('Params :', best_ada.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "          related       0.81      0.96      0.88      6534\n",
      "          request       0.83      0.51      0.63      1472\n",
      "      aid_related       0.76      0.59      0.67      3545\n",
      "     medical_help       0.60      0.19      0.28       701\n",
      " medical_products       0.75      0.23      0.35       446\n",
      "search_and_rescue       0.70      0.10      0.18       226\n",
      "         military       0.64      0.21      0.31       267\n",
      "            water       0.75      0.60      0.67       543\n",
      "             food       0.81      0.69      0.75       965\n",
      "          shelter       0.80      0.50      0.62       775\n",
      "         clothing       0.70      0.35      0.47       127\n",
      "            money       0.54      0.19      0.29       191\n",
      "   missing_people       0.82      0.13      0.23       104\n",
      "         refugees       0.60      0.20      0.30       293\n",
      "            death       0.81      0.37      0.51       406\n",
      "        other_aid       0.62      0.09      0.15      1139\n",
      "        transport       0.75      0.15      0.26       407\n",
      "        buildings       0.80      0.31      0.44       441\n",
      "      electricity       0.66      0.18      0.28       185\n",
      "  weather_related       0.87      0.62      0.72      2390\n",
      "           floods       0.88      0.52      0.65       693\n",
      "            storm       0.75      0.47      0.58       812\n",
      "       earthquake       0.88      0.76      0.82       787\n",
      "             cold       0.76      0.26      0.38       187\n",
      "    direct_report       0.76      0.43      0.55      1694\n",
      "\n",
      "        micro avg       0.80      0.59      0.68     25330\n",
      "        macro avg       0.75      0.38      0.48     25330\n",
      "     weighted avg       0.78      0.59      0.64     25330\n",
      "      samples avg       0.66      0.50      0.53     25330\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_ada.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names = target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Other Approaches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom estimators (inspired by: [repo](https://github.com/hnbezz/Portfolio_under_construction/blob/master/Disaster_Response_Pipeline/ML%20Pipeline%20Preparation.ipynb) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StartVerbExtractor(BaseEstimator, TransformerMixin):\n",
    "    def start_verb(self, text):\n",
    "        sentence_list = nltk.sent_tokenize(text)\n",
    "        for sentence in sentence_list:\n",
    "            pos_tags = nltk.pos_tag(tokenize(sentence))\n",
    "            if len(pos_tags) != 0:\n",
    "                first_word, first_tag = pos_tags[0]\n",
    "                if first_tag in ['VB', 'VBP'] or first_word == 'RT':\n",
    "                    return 1\n",
    "        return 0\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "\n",
    "    def transform(self, X):\n",
    "        X_tag = pd.Series(X).apply(self.start_verb)\n",
    "        return pd.DataFrame(X_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_len(data):\n",
    "    return np.array([len(text) for text in data]).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the targets that had the word performances based on the classification report\n",
    "targs_drop = ['offer', 'security', 'infrastructure_related', 'tools', \n",
    "              'hospitals', 'shops', 'aid_centers', 'other_infrastructure', 'fire', 'other_weather', 'other_aid']\n",
    "y_min = y.copy()\n",
    "y_min.drop(targs_drop, axis = 1, inplace = True)\n",
    "target_names = y_min.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stratifying data\n",
    "mlss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=0.33, random_state=42)\n",
    "\n",
    "for train_index, test_index in mlss.split(X, y_min):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y_min.values[train_index], y_min.values[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.DataFrame(y_train,columns=target_names)\n",
    "y_test = pd.DataFrame(y_test,columns=target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_2 = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "            ('text_pipeline', Pipeline([\n",
    "                ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "                ('best', TruncatedSVD()),\n",
    "                ('tfidf', TfidfTransformer())])), \n",
    "        ('start_verb', StartVerbExtractor())])), \n",
    "    ('clf', MultiOutputClassifier(AdaBoostClassifier()))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None, 'steps': [('features', FeatureUnion(n_jobs=None,\n",
       "                transformer_list=[('text_pipeline',\n",
       "                                   Pipeline(memory=None,\n",
       "                                            steps=[('vect',\n",
       "                                                    CountVectorizer(analyzer='word',\n",
       "                                                                    binary=False,\n",
       "                                                                    decode_error='strict',\n",
       "                                                                    dtype=<class 'numpy.int64'>,\n",
       "                                                                    encoding='utf-8',\n",
       "                                                                    input='content',\n",
       "                                                                    lowercase=True,\n",
       "                                                                    max_df=1.0,\n",
       "                                                                    max_features=None,\n",
       "                                                                    min_df=1,\n",
       "                                                                    ngram_range=(1,\n",
       "                                                                                 1),\n",
       "                                                                    preprocessor=None,\n",
       "                                                                    stop_words=None,\n",
       "                                                                    strip_accents=None,\n",
       "                                                                    tok...\\w+\\\\b',\n",
       "                                                                    tokenizer=<function tokenize at 0x0000028973392D38>,\n",
       "                                                                    vocabulary=None)),\n",
       "                                                   ('best',\n",
       "                                                    TruncatedSVD(algorithm='randomized',\n",
       "                                                                 n_components=2,\n",
       "                                                                 n_iter=5,\n",
       "                                                                 random_state=None,\n",
       "                                                                 tol=0.0)),\n",
       "                                                   ('tfidf',\n",
       "                                                    TfidfTransformer(norm='l2',\n",
       "                                                                     smooth_idf=True,\n",
       "                                                                     sublinear_tf=False,\n",
       "                                                                     use_idf=True))],\n",
       "                                            verbose=False)),\n",
       "                                  ('start_verb', StartVerbExtractor())],\n",
       "                transformer_weights=None, verbose=False)),\n",
       "  ('clf',\n",
       "   MultiOutputClassifier(estimator=AdaBoostClassifier(algorithm='SAMME.R',\n",
       "                                                      base_estimator=None,\n",
       "                                                      learning_rate=1.0,\n",
       "                                                      n_estimators=50,\n",
       "                                                      random_state=None),\n",
       "                         n_jobs=None))], 'verbose': False, 'features': FeatureUnion(n_jobs=None,\n",
       "              transformer_list=[('text_pipeline',\n",
       "                                 Pipeline(memory=None,\n",
       "                                          steps=[('vect',\n",
       "                                                  CountVectorizer(analyzer='word',\n",
       "                                                                  binary=False,\n",
       "                                                                  decode_error='strict',\n",
       "                                                                  dtype=<class 'numpy.int64'>,\n",
       "                                                                  encoding='utf-8',\n",
       "                                                                  input='content',\n",
       "                                                                  lowercase=True,\n",
       "                                                                  max_df=1.0,\n",
       "                                                                  max_features=None,\n",
       "                                                                  min_df=1,\n",
       "                                                                  ngram_range=(1,\n",
       "                                                                               1),\n",
       "                                                                  preprocessor=None,\n",
       "                                                                  stop_words=None,\n",
       "                                                                  strip_accents=None,\n",
       "                                                                  tok...\\w+\\\\b',\n",
       "                                                                  tokenizer=<function tokenize at 0x0000028973392D38>,\n",
       "                                                                  vocabulary=None)),\n",
       "                                                 ('best',\n",
       "                                                  TruncatedSVD(algorithm='randomized',\n",
       "                                                               n_components=2,\n",
       "                                                               n_iter=5,\n",
       "                                                               random_state=None,\n",
       "                                                               tol=0.0)),\n",
       "                                                 ('tfidf',\n",
       "                                                  TfidfTransformer(norm='l2',\n",
       "                                                                   smooth_idf=True,\n",
       "                                                                   sublinear_tf=False,\n",
       "                                                                   use_idf=True))],\n",
       "                                          verbose=False)),\n",
       "                                ('start_verb', StartVerbExtractor())],\n",
       "              transformer_weights=None, verbose=False), 'clf': MultiOutputClassifier(estimator=AdaBoostClassifier(algorithm='SAMME.R',\n",
       "                                                    base_estimator=None,\n",
       "                                                    learning_rate=1.0,\n",
       "                                                    n_estimators=50,\n",
       "                                                    random_state=None),\n",
       "                       n_jobs=None), 'features__n_jobs': None, 'features__transformer_list': [('text_pipeline',\n",
       "   Pipeline(memory=None,\n",
       "            steps=[('vect',\n",
       "                    CountVectorizer(analyzer='word', binary=False,\n",
       "                                    decode_error='strict',\n",
       "                                    dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                    input='content', lowercase=True, max_df=1.0,\n",
       "                                    max_features=None, min_df=1,\n",
       "                                    ngram_range=(1, 1), preprocessor=None,\n",
       "                                    stop_words=None, strip_accents=None,\n",
       "                                    token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                    tokenizer=<function tokenize at 0x0000028973392D38>,\n",
       "                                    vocabulary=None)),\n",
       "                   ('best',\n",
       "                    TruncatedSVD(algorithm='randomized', n_components=2, n_iter=5,\n",
       "                                 random_state=None, tol=0.0)),\n",
       "                   ('tfidf',\n",
       "                    TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                     sublinear_tf=False, use_idf=True))],\n",
       "            verbose=False)),\n",
       "  ('start_verb',\n",
       "   StartVerbExtractor())], 'features__transformer_weights': None, 'features__verbose': False, 'features__text_pipeline': Pipeline(memory=None,\n",
       "          steps=[('vect',\n",
       "                  CountVectorizer(analyzer='word', binary=False,\n",
       "                                  decode_error='strict',\n",
       "                                  dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                  input='content', lowercase=True, max_df=1.0,\n",
       "                                  max_features=None, min_df=1,\n",
       "                                  ngram_range=(1, 1), preprocessor=None,\n",
       "                                  stop_words=None, strip_accents=None,\n",
       "                                  token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                  tokenizer=<function tokenize at 0x0000028973392D38>,\n",
       "                                  vocabulary=None)),\n",
       "                 ('best',\n",
       "                  TruncatedSVD(algorithm='randomized', n_components=2, n_iter=5,\n",
       "                               random_state=None, tol=0.0)),\n",
       "                 ('tfidf',\n",
       "                  TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                   sublinear_tf=False, use_idf=True))],\n",
       "          verbose=False), 'features__start_verb': StartVerbExtractor(), 'features__text_pipeline__memory': None, 'features__text_pipeline__steps': [('vect',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function tokenize at 0x0000028973392D38>,\n",
       "                   vocabulary=None)),\n",
       "  ('best', TruncatedSVD(algorithm='randomized', n_components=2, n_iter=5,\n",
       "                random_state=None, tol=0.0)),\n",
       "  ('tfidf',\n",
       "   TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True))], 'features__text_pipeline__verbose': False, 'features__text_pipeline__vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function tokenize at 0x0000028973392D38>,\n",
       "                 vocabulary=None), 'features__text_pipeline__best': TruncatedSVD(algorithm='randomized', n_components=2, n_iter=5,\n",
       "              random_state=None, tol=0.0), 'features__text_pipeline__tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True), 'features__text_pipeline__vect__analyzer': 'word', 'features__text_pipeline__vect__binary': False, 'features__text_pipeline__vect__decode_error': 'strict', 'features__text_pipeline__vect__dtype': numpy.int64, 'features__text_pipeline__vect__encoding': 'utf-8', 'features__text_pipeline__vect__input': 'content', 'features__text_pipeline__vect__lowercase': True, 'features__text_pipeline__vect__max_df': 1.0, 'features__text_pipeline__vect__max_features': None, 'features__text_pipeline__vect__min_df': 1, 'features__text_pipeline__vect__ngram_range': (1,\n",
       "  1), 'features__text_pipeline__vect__preprocessor': None, 'features__text_pipeline__vect__stop_words': None, 'features__text_pipeline__vect__strip_accents': None, 'features__text_pipeline__vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'features__text_pipeline__vect__tokenizer': <function __main__.tokenize(text)>, 'features__text_pipeline__vect__vocabulary': None, 'features__text_pipeline__best__algorithm': 'randomized', 'features__text_pipeline__best__n_components': 2, 'features__text_pipeline__best__n_iter': 5, 'features__text_pipeline__best__random_state': None, 'features__text_pipeline__best__tol': 0.0, 'features__text_pipeline__tfidf__norm': 'l2', 'features__text_pipeline__tfidf__smooth_idf': True, 'features__text_pipeline__tfidf__sublinear_tf': False, 'features__text_pipeline__tfidf__use_idf': True, 'clf__estimator__algorithm': 'SAMME.R', 'clf__estimator__base_estimator': None, 'clf__estimator__learning_rate': 1.0, 'clf__estimator__n_estimators': 50, 'clf__estimator__random_state': None, 'clf__estimator': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=None), 'clf__n_jobs': None}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_2.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'clf__estimator__n_estimators': [100, 200, 300], \n",
    "              'clf__estimator__random_state': [42],\n",
    "             'clf__estimator__learning_rate': [0.1]} \n",
    "\n",
    "cv_2 = GridSearchCV(pipeline, param_grid = parameters, cv = 10,\n",
    "                  refit = True, verbose = 1, return_train_score = True, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('vect',\n",
       "                                        CountVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.int64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        preprocessor=None,\n",
       "                                                        stop_words=None,\n",
       "                                                        strip_accents=None,\n",
       "                                                        to...\n",
       "                                        MultiOutputClassifier(estimator=AdaBoostClassifier(algorithm='SAMME.R',\n",
       "                                                                                           base_estimator=None,\n",
       "                                                                                           learning_rate=1.0,\n",
       "                                                                                           n_estimators=50,\n",
       "                                                                                           random_state=None),\n",
       "                                                              n_jobs=None))],\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'clf__estimator__learning_rate': [0.1],\n",
       "                         'clf__estimator__n_estimators': [100, 200, 300],\n",
       "                         'clf__estimator__random_state': [42]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "             scoring=None, verbose=1)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 3 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed: 58.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model : 0.2633796401730813\n",
      "Params : {'clf__estimator__learning_rate': 0.1, 'clf__estimator__n_estimators': 300, 'clf__estimator__random_state': 42}\n"
     ]
    }
   ],
   "source": [
    "best_ada_2 = cv_2.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print('Best model :', best_ada_2.best_score_)\n",
    "print('Params :', best_ada_2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "          related       0.79      0.98      0.88      6570\n",
      "          request       0.82      0.49      0.61      1477\n",
      "      aid_related       0.79      0.54      0.65      3584\n",
      "     medical_help       0.66      0.14      0.23       688\n",
      " medical_products       0.71      0.19      0.30       433\n",
      "search_and_rescue       0.63      0.11      0.19       239\n",
      "         military       0.73      0.17      0.28       284\n",
      "            water       0.79      0.64      0.71       552\n",
      "             food       0.78      0.72      0.75       965\n",
      "          shelter       0.86      0.47      0.60       764\n",
      "         clothing       0.76      0.33      0.46       134\n",
      "            money       0.74      0.13      0.21       199\n",
      "   missing_people       0.89      0.16      0.28        98\n",
      "         refugees       0.71      0.17      0.27       289\n",
      "            death       0.79      0.38      0.52       394\n",
      "        transport       0.80      0.16      0.27       396\n",
      "        buildings       0.75      0.26      0.38       440\n",
      "      electricity       0.68      0.17      0.27       176\n",
      "  weather_related       0.89      0.57      0.70      2408\n",
      "           floods       0.92      0.49      0.64       711\n",
      "            storm       0.80      0.47      0.59       806\n",
      "       earthquake       0.90      0.76      0.82       810\n",
      "             cold       0.81      0.22      0.34       175\n",
      "    direct_report       0.79      0.42      0.55      1675\n",
      "\n",
      "        micro avg       0.81      0.61      0.69     24267\n",
      "        macro avg       0.78      0.38      0.48     24267\n",
      "     weighted avg       0.80      0.61      0.66     24267\n",
      "      samples avg       0.70      0.53      0.56     24267\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_ada_2.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names = target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['related' 'weather_related' 'storm']\n"
     ]
    }
   ],
   "source": [
    "test_text = ['there is a storm and people are trapped']\n",
    "test = cv_2.predict(test_text)\n",
    "print(y_train.columns.values[(test.flatten()==1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is a pretty cool prediction, let's try a few more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['related' 'request' 'aid_related' 'buildings' 'weather_related'\n",
      " 'earthquake' 'direct_report']\n"
     ]
    }
   ],
   "source": [
    "test_text = ['we are having an earthquake, buildings are destroyed, victims need clothes']\n",
    "test = cv_2.predict(test_text)\n",
    "print(y_train.columns.values[(test.flatten()==1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['related']\n"
     ]
    }
   ],
   "source": [
    "test_text = ['there was an accident near the bank and we need an ambulance']\n",
    "test = cv_2.predict(test_text)\n",
    "print(y_train.columns.values[(test.flatten()==1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(cv_2, open('classifier.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
